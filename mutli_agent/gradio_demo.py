#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ™ºèƒ½ä½“æ¡†æ¶ - Gradio Web ç•Œé¢ç‰ˆæœ¬
åŸºäºå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆåˆ†ç±»+ä¸“å®¶+è®¨è®ºï¼‰å®ç°æ•…éšœè¯Šæ–­

ä½¿ç”¨æ–¹æ³•ï¼š
    python mutli_agent/gradio_demo.py
    
åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ˜¾ç¤ºçš„ URLï¼ˆé€šå¸¸æ˜¯ http://127.0.0.1:7860ï¼‰
"""

import gradio as gr
import sys
import os
import signal
import json
import re
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥å¤šæ™ºèƒ½ä½“æ¡†æ¶
from mutli_agent import FaultOrchestrator, LLMClient
from cl_agent.monitor_collector import collect_all_metrics, format_metrics_for_display
from cl_agent.agent import export_to_word, export_to_pdf

# å…¨å±€ Orchestrator å®ä¾‹å’Œå½“å‰æ¨¡å‹
orchestrator = None
current_model = "qwen-8b"  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹
# å­˜å‚¨æœ€åä¸€æ¬¡çš„è¯Šæ–­å›å¤ï¼ˆç”¨äºæ–‡æ¡£å¯¼å‡ºï¼‰
last_diagnosis_response = ""
# å…¨å±€ Gradio demo å®ä¾‹ï¼ˆç”¨äºä¼˜é›…å…³é—­ï¼‰
demo_instance = None
# å…³é—­æ ‡å¿—ï¼Œé˜²æ­¢é‡å¤å…³é—­
_shutting_down = False

# æ¨¡å‹åç§°æ˜ å°„ï¼ˆå‰ç«¯æ˜¾ç¤ºåç§° -> å†…éƒ¨æ¨¡å‹åç§°ï¼‰
MODEL_NAME_MAP = {
    "Qwen-8B (vLLM)": "qwen-8b",
    "GPT-4o (OpenAI)": "gpt-4o",
    "DeepSeek-R1 (DeepSeek)": "deepseek-r1"
}


def init_orchestrator(model_name: str = "qwen-8b"):
    """
    åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“åè°ƒå™¨ï¼ˆæ”¯æŒæ¨¡å‹åˆ‡æ¢ï¼‰ 
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œå¯é€‰å€¼ï¼šqwen-8b, gpt-4o, deepseek-r1
    """
    global orchestrator, current_model
    
    model_display_name = {v: k for k, v in MODEL_NAME_MAP.items()}.get(model_name, model_name)
    
    # å¦‚æœæ¨¡å‹æ”¹å˜æˆ–Orchestratoræœªåˆå§‹åŒ–ï¼Œé‡æ–°åˆ›å»º
    if orchestrator is None:
        print(f"[INFO] Orchestratoræœªåˆå§‹åŒ–ï¼Œæ­£åœ¨åˆ›å»ºï¼ˆæ¨¡å‹: {model_display_name}ï¼‰...")
        try:
            llm_client = LLMClient(model_name=model_name)
            orchestrator = FaultOrchestrator(llm_client, model_name=model_name)
            current_model = model_name
            print(f"[INFO] âœ… å¤šæ™ºèƒ½ä½“åè°ƒå™¨åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡å‹: {model_display_name}ï¼‰")
        except Exception as e:
            error_msg = f"å¤šæ™ºèƒ½ä½“åè°ƒå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}"
            print(f"[ERROR] {error_msg}")
            raise RuntimeError(error_msg)
    elif current_model != model_name:
        print(f"[INFO] æ£€æµ‹åˆ°æ¨¡å‹åˆ‡æ¢è¯·æ±‚:")
        print(f"[INFO]   å½“å‰æ¨¡å‹: {current_model} ({MODEL_NAME_MAP.get(current_model, current_model)})")
        print(f"[INFO]   ç›®æ ‡æ¨¡å‹: {model_name} ({model_display_name})")
        print(f"[INFO] æ­£åœ¨é‡æ–°åˆ›å»ºOrchestrator...")
        try:
            llm_client = LLMClient(model_name=model_name)
            orchestrator = FaultOrchestrator(llm_client, model_name=model_name)
            old_model = current_model
            current_model = model_name
            print(f"[INFO] âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ:")
            print(f"[INFO]   ä»: {old_model} ({MODEL_NAME_MAP.get(old_model, old_model)})")
            print(f"[INFO]   åˆ°: {current_model} ({model_display_name})")
        except Exception as e:
            error_msg = f"æ¨¡å‹åˆ‡æ¢å¤±è´¥: {str(e)}"
            print(f"[ERROR] {error_msg}")
            raise RuntimeError(error_msg)
    else:
        print(f"[DEBUG] ä½¿ç”¨ç°æœ‰Orchestratorï¼ˆæ¨¡å‹: {model_display_name}ï¼‰")
    
    return orchestrator


def update_monitoring_display():
    """æ›´æ–°ç›‘æ§æ•°æ®æ˜¾ç¤º"""
    try:
        metrics_data = collect_all_metrics()
        html_content = format_metrics_for_display(metrics_data)
        return html_content
    except Exception as e:
        error_html = f"<div style='color: red; padding: 20px;'>âŒ è·å–ç›‘æ§æ•°æ®å¤±è´¥: {str(e)}</div>"
        return error_html


def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è‡ªå®šä¹‰ CSS æ ·å¼
    custom_css = """
    .gradio-container {
        font-family: 'Microsoft YaHei', 'PingFang SC', Arial, sans-serif;
    }
    .chat-message {
        padding: 10px;
    }
    """
    
    # ä½¿ç”¨ Blocks åˆ›å»ºæ›´çµæ´»çš„å¸ƒå±€
    with gr.Blocks(title="Hadoop é›†ç¾¤ç›‘æ§ Agent (å¤šæ™ºèƒ½ä½“æ¡†æ¶)", theme=gr.themes.Soft(), css=custom_css) as demo:
        # ä½¿ç”¨ä¸¤åˆ—å¸ƒå±€ï¼šå·¦ä¾§ï¼ˆæ ‡é¢˜+ç›‘æ§ï¼‰ï¼Œå³ä¾§ï¼ˆèŠå¤©ï¼‰
        with gr.Row():
            # å·¦ä¾§åˆ—ï¼šæ ‡é¢˜+åŠŸèƒ½è¯´æ˜ + ç›‘æ§æ•°æ®
            with gr.Column(scale=1, min_width=300):
                # æ ‡é¢˜å’ŒåŠŸèƒ½è¯´æ˜
                gr.Markdown("""
                #  Hadoop é›†ç¾¤ç›‘æ§æ™ºèƒ½åŠ©æ‰‹
                
                **å¤šæ™ºèƒ½ä½“æ¡†æ¶**ï¼š
                - ğŸ“‹ **åˆ†ç±»Agent**ï¼šè‡ªåŠ¨è¯†åˆ«æ•…éšœç±»å‹
                - ğŸ” **ä¸“å®¶Agent**ï¼šå¤šä¸“å®¶å¹¶è¡Œè¯Šæ–­ï¼ˆHDFS/YARN/MapReduce/Networkï¼‰
                - ğŸ’¬ **è®¨è®ºAgent**ï¼šç»¼åˆä¸“å®¶æ„è§ï¼Œç”Ÿæˆæœ€ç»ˆè¯Šæ–­
                - ğŸ“Š **å®æ—¶ç›‘æ§**ï¼šæ˜¾ç¤ºé›†ç¾¤å…³é”®æŒ‡æ ‡
                - ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒåˆ‡æ¢ Qwen-8Bã€GPT-4oã€DeepSeek-R1
                
                **è¯Šæ–­æµç¨‹**ï¼š
                1. æ”¶é›†å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆæ—¥å¿—+ç›‘æ§ï¼‰
                2. åˆ†ç±»Agentè¯†åˆ«æ•…éšœç±»å‹
                3. é€‰æ‹©ç›¸å…³ä¸“å®¶å¹¶è¡Œè¯Šæ–­
                4. Discussion Agentç»¼åˆç»“æœ
                """)
                
                gr.Markdown("### ğŸ“Š é›†ç¾¤ç›‘æ§æŒ‡æ ‡")
                monitoring_html = gr.HTML(
                    value="<div style='padding: 20px; text-align: center;'>æ­£åœ¨åŠ è½½ç›‘æ§æ•°æ®...</div>",
                    label="ç›‘æ§æ•°æ®",
                    elem_id="monitoring-display"
                )
                
                # åˆ·æ–°æŒ‰é’®
                refresh_btn = gr.Button("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°", variant="primary", size="sm")
                
                def refresh_monitoring():
                    """æ‰‹åŠ¨åˆ·æ–°ç›‘æ§æ•°æ®""" 
                    return update_monitoring_display()
                
                refresh_btn.click(
                    fn=refresh_monitoring,
                    inputs=None,
                    outputs=monitoring_html
                )
                
                # å¯¼å‡ºæŒ‰é’®
                with gr.Row():
                    export_word_btn = gr.Button("ğŸ“„ å¯¼å‡ºWord", variant="secondary", size="sm", scale=1)
                    export_pdf_btn = gr.Button("ğŸ“„ å¯¼å‡ºPDF", variant="secondary", size="sm", scale=1)
                
                # å¯¼å‡ºçŠ¶æ€
                export_status = gr.Textbox(
                    label="å¯¼å‡ºçŠ¶æ€",
                    visible=True,
                    interactive=False,
                    lines=2
                )
                
                # æ–‡ä»¶ä¸‹è½½ç»„ä»¶
                export_file = gr.File(
                    label="ä¸‹è½½æ–‡ä»¶",
                    visible=True,
                    interactive=False,
                    height=50
                )
            
            # å³ä¾§åˆ—ï¼šèŠå¤©å¯¹è¯
            with gr.Column(scale=2, min_width=400):
                gr.Markdown("### ğŸ’¬ å¤šæ™ºèƒ½ä½“è¯Šæ–­ï¼ˆåˆ†ç±»â†’ä¸“å®¶â†’è®¨è®ºï¼‰")
                
                # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                model_selector = gr.Dropdown(
                    choices=list(MODEL_NAME_MAP.keys()),
                    value="Qwen-8B (vLLM)",
                    label="é€‰æ‹©å¤§æ¨¡å‹",
                    info="åˆ‡æ¢ä¸åŒçš„å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯",
                    interactive=True
                )
                
                chatbot = gr.Chatbot(
                    label="å¯¹è¯å†å²",
                    height=500,
                    show_copy_button=True
                )
                msg = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæŸ¥çœ‹é›†ç¾¤çŠ¶æ€ã€åˆ†ææ˜¯å¦æœ‰æ•…éšœã€æ£€æŸ¥DataNodeçŠ¶æ€",
                    lines=2
                )
                
                with gr.Row():
                    submit_btn = gr.Button("ğŸ“¤ å‘é€", variant="primary")
                    clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                
                # ç¤ºä¾‹é—®é¢˜
                gr.Examples(
                    examples=[
                        "æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ï¼Œåˆ†ææ˜¯å¦æœ‰æ•…éšœ",
                        "æ£€æŸ¥DataNodeæ˜¯å¦æ­£å¸¸",
                        "åˆ†æYARNä»»åŠ¡å¤±è´¥çš„åŸå› ",
                        "è¯Šæ–­MapReduceä»»åŠ¡å†…å­˜ä¸è¶³é—®é¢˜",
                    ],
                    inputs=msg
                )
        
        # æ¨¡å‹åˆ‡æ¢åŠŸèƒ½
        def switch_model(selected_model, chat_history):
            """åˆ‡æ¢æ¨¡å‹å¹¶æ¸…ç©ºå¯¹è¯å†å²"""
            global orchestrator, current_model
            
            # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åœ¨æ˜ å°„ä¸­
            if selected_model not in MODEL_NAME_MAP:
                error_msg = f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {selected_model}ï¼Œè¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹"
                print(f"[ERROR] {error_msg}")
                if chat_history:
                    chat_history.append(["", error_msg])
                else:
                    chat_history = [["", error_msg]]
                return chat_history
            
            model_name = MODEL_NAME_MAP[selected_model]
            
            if current_model != model_name:
                try:
                    orchestrator = None
                    init_orchestrator(model_name)
                    return []
                except Exception as e:
                    error_msg = f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}"
                    if chat_history:
                        chat_history.append(["", error_msg])
                    else:
                        chat_history = [["", error_msg]]
                    return chat_history
            return chat_history
        
        # ç»‘å®šæ¨¡å‹åˆ‡æ¢äº‹ä»¶
        model_selector.change(
            fn=switch_model,
            inputs=[model_selector, chatbot],
            outputs=[chatbot]
        )
        
        # èŠå¤©åŠŸèƒ½
        def respond(message, chat_history, selected_model):
            """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
            global last_diagnosis_response
            
            if not message.strip():
                return chat_history, ""
            
            # ç¬¬ä¸€æ­¥ï¼šç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤º"æ­£åœ¨å¤„ç†..."æç¤º
            chat_history.append([message, "â³ æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."])
            yield chat_history, ""
            
            # ç¬¬äºŒæ­¥ï¼šè·å–è¯Šæ–­ç»“æœ
            try:
                # æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦åœ¨æ˜ å°„ä¸­
                if selected_model not in MODEL_NAME_MAP:
                    error_msg = f"âŒ æœªçŸ¥çš„æ¨¡å‹é€‰æ‹©: {selected_model}ï¼Œè¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹"
                    print(f"[ERROR] {error_msg}")
                    if len(chat_history) > 0:
                        if isinstance(chat_history[-1], list) and len(chat_history[-1]) >= 2:
                            chat_history[-1][1] = error_msg
                        else:
                            chat_history[-1] = [message, error_msg]
                    else:
                        chat_history.append([message, error_msg])
                    yield chat_history, ""
                    return
                
                model_name = MODEL_NAME_MAP[selected_model]
                print(f"[DEBUG] ========== å¤„ç†ç”¨æˆ·æ¶ˆæ¯ ==========")
                print(f"[DEBUG] ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹: {selected_model} -> {model_name}")
                print(f"[DEBUG] è°ƒç”¨ init_orchestrator('{model_name}') è·å–åè°ƒå™¨...")
                current_orchestrator = init_orchestrator(model_name)
                print(f"[DEBUG] âœ… åè°ƒå™¨è·å–æˆåŠŸï¼Œå¼€å§‹è¯Šæ–­...")
                
                # æ‰§è¡Œè¯Šæ–­ï¼ˆè¿”å›å¯¹è¯å¼æ–‡æœ¬ï¼‰
                response = current_orchestrator.diagnose(message)
                
                # ç¡®ä¿responseæ˜¯å­—ç¬¦ä¸²
                if not isinstance(response, str):
                    response = str(response)
                
                # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                response = response.strip()
                
                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                if not response or not response.strip():
                    response = "âš ï¸ è¯Šæ–­è¿”å›äº†ç©ºå“åº”ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚"
                    print("[ERROR] è¯Šæ–­è¿”å›äº†ç©ºå“åº”")
                
                last_diagnosis_response = response
                
                # æ›´æ–°å¯¹è¯å†å²
                if len(chat_history) > 0:
                    if isinstance(chat_history[-1], list) and len(chat_history[-1]) >= 2:
                        chat_history[-1][1] = response
                    else:
                        chat_history[-1] = [message, response]
                else:
                    chat_history.append([message, response])
                
            except Exception as e:
                error_msg = f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"
                last_diagnosis_response = error_msg
                print(f"[ERROR] {error_msg}")
                import traceback
                traceback.print_exc()
                
                if len(chat_history) > 0:
                    if isinstance(chat_history[-1], list) and len(chat_history[-1]) >= 2:
                        chat_history[-1][1] = error_msg
                    else:
                        chat_history[-1] = [message, error_msg]
                else:
                    chat_history.append([message, error_msg])
            
            # ç¬¬ä¸‰æ­¥ï¼šè¿”å›å®Œæ•´çš„å¯¹è¯å†å²
            yield chat_history, ""
        
        # æ–‡æ¡£å¯¼å‡ºåŠŸèƒ½
        def export_document(format_type: str):
            """å¯¼å‡ºæ–‡æ¡£å¹¶è¿”å›æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæµè§ˆå™¨ä¸‹è½½ï¼‰"""
            global last_diagnosis_response
            
            if not last_diagnosis_response or last_diagnosis_response.startswith("âŒ"):
                return None, "âŒ æ²¡æœ‰å¯å¯¼å‡ºçš„å†…å®¹ï¼Œè¯·å…ˆä¸Agentå¯¹è¯è·å–åˆ†æç»“æœ"
            
            try:
                current_work_dir = os.getcwd()
                exports_dir = os.path.join(current_work_dir, "exports")
                os.makedirs(exports_dir, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                if format_type == "word":
                    filename = f"multi_agent_report_{timestamp}.docx"
                    output_path = os.path.join(exports_dir, filename)
                    export_to_word(last_diagnosis_response, output_path)
                    status_msg = f"âœ… Wordæ–‡æ¡£å·²ç”Ÿæˆï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ä¸‹è½½é“¾æ¥"
                    rel_path = os.path.relpath(output_path, current_work_dir)
                    return rel_path, status_msg
                else:  # pdf
                    filename = f"multi_agent_report_{timestamp}.pdf"
                    output_path = os.path.join(exports_dir, filename)
                    export_to_pdf(last_diagnosis_response, output_path)
                    status_msg = f"âœ… PDFæ–‡æ¡£å·²ç”Ÿæˆï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹ä¸‹è½½é“¾æ¥"
                    rel_path = os.path.relpath(output_path, current_work_dir)
                    return rel_path, status_msg
            except Exception as e:
                return None, f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}"
        
        msg.submit(respond, [msg, chatbot, model_selector], [chatbot, msg])
        submit_btn.click(respond, [msg, chatbot, model_selector], [chatbot, msg])
        clear_btn.click(lambda: ([], ""), None, [chatbot, msg])
        
        # ç»‘å®šå¯¼å‡ºæŒ‰é’®
        export_word_btn.click(
            fn=lambda: export_document("word"),
            inputs=None,
            outputs=[export_file, export_status],
            js="""
            (file, status) => {
                if (file) {
                    setTimeout(() => {
                        // æŸ¥æ‰¾æ–‡ä»¶ä¸‹è½½é“¾æ¥å¹¶è‡ªåŠ¨ç‚¹å‡»
                        const fileLinks = document.querySelectorAll('a[href*="multi_agent_report"]');
                        if (fileLinks.length > 0) {
                            fileLinks[fileLinks.length - 1].click();
                        } else {
                            // å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥åˆ›å»ºä¸‹è½½é“¾æ¥
                            const link = document.createElement('a');
                            link.href = file;
                            link.download = file.split('/').pop() || 'multi_agent_report.docx';
                            link.style.display = 'none';
                            document.body.appendChild(link);
                            link.click();
                            setTimeout(() => document.body.removeChild(link), 100);
                        }
                    }, 500);
                }
                return [file, status];
            }
            """
        )
        export_pdf_btn.click(
            fn=lambda: export_document("pdf"),
            inputs=None,
            outputs=[export_file, export_status],
            js="""
            (file, status) => {
                if (file) {
                    setTimeout(() => {
                        // æŸ¥æ‰¾æ–‡ä»¶ä¸‹è½½é“¾æ¥å¹¶è‡ªåŠ¨ç‚¹å‡»
                        const fileLinks = document.querySelectorAll('a[href*="multi_agent_report"]');
                        if (fileLinks.length > 0) {
                            fileLinks[fileLinks.length - 1].click();
                        } else {
                            // å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥åˆ›å»ºä¸‹è½½é“¾æ¥
                            const link = document.createElement('a');
                            link.href = file;
                            link.download = file.split('/').pop() || 'multi_agent_report.pdf';
                            link.style.display = 'none';
                            document.body.appendChild(link);
                            link.click();
                            setTimeout(() => document.body.removeChild(link), 100);
                        }
                    }, 500);
                }
                return [file, status];
            }
            """
        )
        
        # é¡µé¢åŠ è½½æ—¶ç«‹å³æ›´æ–°ä¸€æ¬¡
        demo.load(
            fn=update_monitoring_display,
            inputs=None,
            outputs=monitoring_html
        )
    
    return demo


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Hadoop é›†ç¾¤ç›‘æ§ Agent - Gradio Web ç•Œé¢ (å¤šæ™ºèƒ½ä½“æ¡†æ¶)")
    print("=" * 60)
    print()
    print("[INFO] ä½¿ç”¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆåˆ†ç±»â†’ä¸“å®¶â†’è®¨è®ºï¼‰")
    print("[INFO] æ¨¡å¼: vLLMï¼ˆQwen3-8Bï¼‰")
    print()
    print("[æç¤º] vLLM é…ç½®ï¼š")
    print("[æç¤º]   - æœåŠ¡åœ°å€: http://10.157.197.76:8001/v1")
    print("[æç¤º]   - æ¨¡å‹è·¯å¾„: /media/hnu/LLM/hnu/LLM/Qwen3-8B")
    print()
    print("[æç¤º] è¯·ç¡®ä¿ vLLM æœåŠ¡å·²å¯åŠ¨")
    print("-" * 60)
    print()
    print("[INFO] æ­£åœ¨é¢„åŠ è½½ Orchestrator...")
    print("[æç¤º] è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("-" * 60)
    
    # åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½ Orchestratorï¼ˆä½¿ç”¨é»˜è®¤æ¨¡å‹ qwen-8bï¼‰
    try:
        init_orchestrator("qwen-8b")
        print("[INFO] Orchestrator é¢„åŠ è½½å®Œæˆï¼ˆé»˜è®¤æ¨¡å‹: Qwen3-8Bï¼‰ï¼")
    except Exception as e:
        print(f"[ERROR] Orchestrator é¢„åŠ è½½å¤±è´¥: {e}")
        print()
        print("[é”™è¯¯] æ— æ³•å¯åŠ¨ Orchestratorï¼Œè¯·æ£€æŸ¥ï¼š")
        print("  1. vLLM æœåŠ¡æ˜¯å¦å·²å¯åŠ¨")
        print("  2. æœåŠ¡åœ°å€æ˜¯å¦æ­£ç¡®")
        print("  3. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print()
        print("[æç¤º] å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ vLLM æœåŠ¡ï¼š")
        print("  curl http://10.157.197.76:8001/health")
        print()
        print("[æç¤º] å¦‚æœ vLLM æœåŠ¡ä¸å¯ç”¨ï¼Œå¯ä»¥åœ¨ç•Œé¢ä¸­åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹ï¼ˆGPT-4o æˆ– DeepSeek-R1ï¼‰")
        print()
    
    print()
    print("[INFO] æ­£åœ¨å¯åŠ¨ Gradio ç•Œé¢...")
    print("[INFO] ç•Œé¢å¯åŠ¨åï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ˜¾ç¤ºçš„ URL")
    print("-" * 60)
    print()
    
    # å®šä¹‰ä¼˜é›…å…³é—­å‡½æ•°
    def graceful_shutdown(signum=None, frame=None):
        """ä¼˜é›…å…³é—­ Gradio æœåŠ¡å™¨"""
        global demo_instance, _shutting_down
        
        if _shutting_down:
            os._exit(0)
            return
        
        _shutting_down = True
        print("\n[INFO] æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...") 
        
        import threading
        def close_demo():
            if demo_instance is not None:
                try:
                    demo_instance.close()
                    print("[INFO] Gradio æœåŠ¡å™¨å·²å…³é—­")
                except Exception as e:
                    print(f"[WARNING] å…³é—­æœåŠ¡å™¨æ—¶å‡ºé”™: {e}")
            import time
            time.sleep(0.1)
            os._exit(0)
        
        close_thread = threading.Thread(target=close_demo, daemon=True)
        close_thread.start()
        os._exit(0)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, graceful_shutdown)
    if sys.platform != "win32":
        signal.signal(signal.SIGTERM, graceful_shutdown)
    
    try:
        # åˆ›å»ºç•Œé¢
        demo = create_gradio_interface()
        global demo_instance
        demo_instance = demo
        
        # å¯åŠ¨ç•Œé¢
        current_work_dir = os.getcwd()
        exports_dir = os.path.join(current_work_dir, "exports")
        exports_dir_abs = os.path.abspath(exports_dir)
        os.makedirs(exports_dir_abs, exist_ok=True)
        
        print(f"[DEBUG] Gradioå·¥ä½œç›®å½•: {current_work_dir}")
        print(f"[DEBUG] Exportsç›®å½•: {exports_dir_abs}")
        
        demo.launch(
            share=False,
            server_name="127.0.0.1",
            server_port=7860,
            show_error=True,
            allowed_paths=[exports_dir_abs, current_work_dir]
        )
    except KeyboardInterrupt:
        graceful_shutdown()
    except Exception as e:
        print(f"\n[ERROR] å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        graceful_shutdown()
        sys.exit(1)


if __name__ == "__main__":
    main()
