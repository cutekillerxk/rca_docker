#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†ç¾¤ä¸Šä¸‹æ–‡é…ç½®æ¨¡å—
åŒ…å«Hadoopé›†ç¾¤çš„å®Œæ•´é…ç½®ä¿¡æ¯ï¼Œä¾›Agentçš„System Promptä½¿ç”¨

é…ç½®ä¿¡æ¯æ¥æºï¼š
- docker-compose.yml
- å®¹å™¨å†…å®é™…é…ç½®æ–‡ä»¶ (core-site.xml, hdfs-site.xmlç­‰)
- è¿è¡Œæ—¶æŸ¥è¯¢ (hdfs getconf, hdfs dfsadmin -report)

æœ€åæ›´æ–°ï¼š2026-01-08
"""

# ==================== åŸºç¡€è®¾æ–½å±‚ (Infrastructure) ====================
# æè¿°é›†ç¾¤çš„ç‰©ç†/è™šæ‹Ÿèµ„æºéƒ¨ç½²æƒ…å†µ

INFRASTRUCTURE = {
    # éƒ¨ç½²æ–¹å¼
    "deployment": {
        "type": "Docker Compose",
        "description": "åŸºäºDocker Composeçš„å®¹å™¨åŒ–éƒ¨ç½²ï¼Œæ‰€æœ‰HadoopæœåŠ¡è¿è¡Œåœ¨Dockerå®¹å™¨å†…",
        "compose_file": "docker-compose.yml",
    },
    
    # Dockerç½‘ç»œé…ç½®
    "network": {
        "name": "hadoop-network",
        "driver": "bridge",
        "description": "æ‰€æœ‰å®¹å™¨åœ¨åŒä¸€bridgeç½‘ç»œä¸­ï¼Œå¯é€šè¿‡å®¹å™¨å(hostname)äº’ç›¸è®¿é—®",
    },
    
    # èŠ‚ç‚¹/å®¹å™¨åˆ—è¡¨
    "nodes": {
        "namenode": {
            "container_name": "namenode",
            "hostname": "namenode",
            "image": "cutekiller/myhadoop-namenode:v1",
            "role": "ä¸»èŠ‚ç‚¹ï¼Œè¿è¡ŒNameNodeã€DataNodeã€SecondaryNameNode",
            "services": ["NameNode", "DataNode", "SecondaryNameNode"],
            "ports": {
                "namenode_webui": {"host": 9870, "container": 9870, "description": "NameNode Web UI"},
                "hdfs_rpc": {"host": 9000, "container": 9000, "description": "HDFS RPCç«¯å£"},
                "hdfs_rpc_alt": {"host": 8020, "container": 8020, "description": "HDFS RPCå¤‡ç”¨ç«¯å£"},
                "datanode_webui": {"host": 9866, "container": 9864, "description": "DataNode Web UI (namenodeå®¹å™¨å†…)"},
                "ssh": {"host": 2225, "container": 22, "description": "SSHç«¯å£"},
            },
        },
        "datanode1": {
            "container_name": "datanode1",
            "hostname": "datanode1",
            "image": "cutekiller/myhadoop-datanode1:v1",
            "role": "æ•°æ®èŠ‚ç‚¹1ï¼Œè¿è¡ŒDataNode",
            "services": ["DataNode"],
            "ports": {
                "datanode_webui": {"host": 9864, "container": 9864, "description": "DataNode Web UI"},
                "ssh": {"host": 2223, "container": 22, "description": "SSHç«¯å£"},
            },
        },
        "datanode2": {
            "container_name": "datanode2",
            "hostname": "datanode2",
            "image": "cutekiller/myhadoop-datanode2:v1",
            "role": "æ•°æ®èŠ‚ç‚¹2ï¼Œè¿è¡ŒDataNode",
            "services": ["DataNode"],
            "ports": {
                "datanode_webui": {"host": 9865, "container": 9864, "description": "DataNode Web UI"},
                "ssh": {"host": 2224, "container": 22, "description": "SSHç«¯å£"},
            },
        },
    },
    
    # å®¹å™¨ç™½åå•ï¼ˆå…è®¸æ“ä½œçš„å®¹å™¨ï¼‰
    "allowed_containers": ["namenode", "datanode1", "datanode2"],
}


# ==================== ç»„ä»¶é…ç½®å±‚ (Components) ====================
# æè¿°Hadoopå„ç»„ä»¶çš„é…ç½®

COMPONENTS = {
    # è¿è¡Œç¯å¢ƒ
    "runtime": {
        "java_version": "OpenJDK 11.0.29",
        "java_home": "/usr/lib/jvm/java-11-openjdk-amd64",
        "hadoop_version": "3.3.6",
        "hadoop_home": "/usr/local/hadoop",
        "hadoop_conf_dir": "/usr/local/hadoop/etc/hadoop",
        "hadoop_user": "hadoop",  # é‡è¦ï¼šHadoopæœåŠ¡ä»¥hadoopç”¨æˆ·è¿è¡Œ
    },
    
    # HDFSé…ç½® (æ¥è‡ª core-site.xml å’Œ hdfs-site.xml)
    "hdfs": {
        "fs_default_fs": "hdfs://namenode:9000",
        "replication": 2,
        "blocksize": 134217728,  # 128MB
        "blocksize_human": "128MB",
        "namenode_dir": "/usr/local/hadoop/hdfs/namenode",
        "datanode_dir": "/usr/local/hadoop/hdfs/datanode",
        "heartbeat_interval": 3,  # ç§’
    },
    
    # workersæ–‡ä»¶å†…å®¹
    "workers": ["namenode", "datanode1", "datanode2"],
    
    # æœŸæœ›çš„é›†ç¾¤çŠ¶æ€
    "expected_state": {
        "total_datanodes": 3,  # åŒ…æ‹¬namenodeå®¹å™¨å†…çš„DataNode
        "live_datanodes": 3,
        "dead_datanodes": 0,
        "missing_blocks": 0,
        "corrupt_blocks": 0,
    },
    
    # YARNé…ç½®ï¼ˆå½“å‰æœªå¯ç”¨ï¼‰
    "yarn": {
        "enabled": False,
        "description": "YARNæœªé…ç½®ï¼Œå½“å‰åªè¿è¡ŒHDFSæœåŠ¡",
    },
    
    # JMXç›‘æ§ç«¯ç‚¹ï¼ˆä»å®¿ä¸»æœºè®¿é—®ï¼‰
    "jmx_endpoints": {
        "namenode": "http://localhost:9870/jmx",
        "datanode1": "http://127.0.0.1:9864/jmx",
        "datanode2": "http://127.0.0.1:9865/jmx",
        "datanode_namenode": "http://127.0.0.1:9866/jmx",  # namenodeå®¹å™¨å†…çš„DataNode
    },
    
    # æ—¥å¿—é…ç½®
    "logs": {
        "log_dir": "/usr/local/hadoop/logs",
        "log_pattern": "hadoop-hadoop-{service_type}-{hostname}.log",
        "files": {
            "namenode": {
                "container": "namenode",
                "files": [
                    "hadoop-hadoop-namenode-namenode.log",
                    "hadoop-hadoop-datanode-namenode.log",
                    "hadoop-hadoop-secondarynamenode-namenode.log",
                ],
            },
            "datanode1": {
                "container": "datanode1",
                "files": ["hadoop-hadoop-datanode-datanode1.log"],
            },
            "datanode2": {
                "container": "datanode2",
                "files": ["hadoop-hadoop-datanode-datanode2.log"],
            },
        },
    },
}


# ==================== æ“ä½œå±‚ (Operations) ====================
# æè¿°å¦‚ä½•æ‰§è¡Œå‘½ä»¤å’Œæ“ä½œ

OPERATIONS = {
    # é‡è¦è¯´æ˜ï¼šç”¨æˆ·æƒé™
    "user_context": {
        "description": "Hadoopé›†ç¾¤ç”±hadoopç”¨æˆ·éƒ¨ç½²å’Œè¿è¡Œï¼Œdocker execé»˜è®¤ä»¥rootç™»å½•ï¼Œå¿…é¡»åˆ‡æ¢ç”¨æˆ·",
        "default_docker_user": "root",
        "hadoop_user": "hadoop",
        "switch_user_required": True,
    },
    
    # å‘½ä»¤æ‰§è¡Œæ ¼å¼æ¨¡æ¿
    "command_templates": {
        # æ ‡å‡†æ ¼å¼ï¼šåœ¨å®¹å™¨å†…ä»¥hadoopç”¨æˆ·æ‰§è¡Œå‘½ä»¤
        "standard": {
            "template": "docker exec {container} sh -c 'su - hadoop -c \"{command}\"'",
            "description": "åœ¨æŒ‡å®šå®¹å™¨å†…åˆ‡æ¢åˆ°hadoopç”¨æˆ·æ‰§è¡Œå‘½ä»¤",
            "components": {
                "docker exec {container}": "åœ¨æŒ‡å®šå®¹å™¨å†…æ‰§è¡Œ",
                "sh -c '...'": "å¯åŠ¨shellæ‰§è¡Œå‘½ä»¤å­—ç¬¦ä¸²",
                "su - hadoop": "åˆ‡æ¢åˆ°hadoopç”¨æˆ·ï¼ˆ'-'ç¡®ä¿åŠ è½½ç¯å¢ƒå˜é‡ï¼‰",
                "-c \"{command}\"": "æ‰§è¡Œå®é™…çš„Hadoopå‘½ä»¤",
            },
        },
        # ç®€åŒ–æ ¼å¼ï¼ˆç›´æ¥æŒ‡å®šç”¨æˆ·ï¼ŒæŸäº›åœºæ™¯å¯ç”¨ï¼‰
        "direct_user": {
            "template": "docker exec -u hadoop {container} {command}",
            "description": "ç›´æ¥ä»¥hadoopç”¨æˆ·èº«ä»½æ‰§è¡Œï¼ˆæ³¨æ„ï¼šå¯èƒ½ä¸åŠ è½½å®Œæ•´ç¯å¢ƒå˜é‡ï¼‰",
        },
        # ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼ˆå½“PATHæœªè®¾ç½®æ—¶ï¼‰
        "full_path": {
            "template": "docker exec {container} sh -c '/usr/local/hadoop/bin/{command}'",
            "description": "ä½¿ç”¨Hadoopå‘½ä»¤çš„å®Œæ•´è·¯å¾„",
        },
    },
    
    # å¸¸ç”¨å‘½ä»¤ç¤ºä¾‹
    "command_examples": {
        # æŸ¥è¯¢ç±»å‘½ä»¤
        "cluster_report": {
            "description": "æŸ¥çœ‹HDFSé›†ç¾¤çŠ¶æ€æŠ¥å‘Š",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfsadmin -report"\'',
        },
        "safemode_get": {
            "description": "æ£€æŸ¥NameNodeå®‰å…¨æ¨¡å¼çŠ¶æ€",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfsadmin -safemode get"\'',
        },
        "safemode_leave": {
            "description": "é€€å‡ºNameNodeå®‰å…¨æ¨¡å¼",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfsadmin -safemode leave"\'',
        },
        "list_hdfs_root": {
            "description": "åˆ—å‡ºHDFSæ ¹ç›®å½•",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfs -ls /"\'',
        },
        "check_java_processes": {
            "description": "æŸ¥çœ‹Javaè¿›ç¨‹ï¼ˆç¡®è®¤æœåŠ¡è¿è¡ŒçŠ¶æ€ï¼‰",
            "command": 'docker exec {container} sh -c \'su - hadoop -c "jps"\'',
        },
        
        # å•èŠ‚ç‚¹æœåŠ¡ç®¡ç†
        "start_datanode": {
            "description": "å¯åŠ¨DataNodeæœåŠ¡",
            "command": 'docker exec {container} sh -c \'su - hadoop -c "hdfs --daemon start datanode"\'',
        },
        "stop_datanode": {
            "description": "åœæ­¢DataNodeæœåŠ¡",
            "command": 'docker exec {container} sh -c \'su - hadoop -c "hdfs --daemon stop datanode"\'',
        },
        "start_namenode": {
            "description": "å¯åŠ¨NameNodeæœåŠ¡",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs --daemon start namenode"\'',
        },
        "stop_namenode": {
            "description": "åœæ­¢NameNodeæœåŠ¡",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs --daemon stop namenode"\'',
        },
        
        # é›†ç¾¤çº§æ“ä½œ
        "start_dfs": {
            "description": "å¯åŠ¨æ•´ä¸ªHDFSé›†ç¾¤ï¼ˆåœ¨namenodeæ‰§è¡Œï¼Œä¼šSSHåˆ°å…¶ä»–èŠ‚ç‚¹ï¼‰",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "start-dfs.sh"\'',
        },
        "stop_dfs": {
            "description": "åœæ­¢æ•´ä¸ªHDFSé›†ç¾¤",
            "command": 'docker exec namenode sh -c \'su - hadoop -c "stop-dfs.sh"\'',
        },
    },
    
    # å®¹å™¨åˆ°HadoopæœåŠ¡ç±»å‹æ˜ å°„
    "container_to_daemon": {
        "namenode": "namenode",  # namenodeå®¹å™¨ä¸Šè¿è¡Œçš„ä¸»è¦æœåŠ¡æ˜¯namenode
        "datanode1": "datanode",
        "datanode2": "datanode",
    },
    
    # å…è®¸çš„æ“ä½œç±»å‹
    "allowed_operations": ["start", "stop", "restart"],
}


# ==================== è¯Šæ–­å±‚ (Diagnostics) ====================
# æè¿°å¦‚ä½•è¯Šæ–­é—®é¢˜

DIAGNOSTICS = {
    # æ—¥å¿—å…³é”®å­—
    "log_keywords": {
        "error_levels": ["ERROR", "FATAL", "EXCEPTION", "CRITICAL"],
        "warning_levels": ["WARN", "WARNING"],
        "important_patterns": {
            "Incompatible clusterIDs": "é›†ç¾¤IDä¸åŒ¹é…ï¼ŒDataNodeä¸NameNodeçš„clusterIDä¸ä¸€è‡´",
            "Connection refused": "è¿æ¥è¢«æ‹’ç»ï¼ŒæœåŠ¡å¯èƒ½æœªå¯åŠ¨æˆ–ç½‘ç»œé—®é¢˜",
            "No space left": "ç£ç›˜ç©ºé—´ä¸è¶³",
            "Safe mode": "NameNodeå¤„äºå®‰å…¨æ¨¡å¼",
            "dead": "èŠ‚ç‚¹ç¦»çº¿æˆ–å¿ƒè·³è¶…æ—¶",
            "removed": "èŠ‚ç‚¹è¢«ç§»é™¤",
            "UnderReplicatedBlocks": "å‰¯æœ¬æ•°ä¸è¶³çš„æ•°æ®å—",
            "MissingBlocks": "ä¸¢å¤±çš„æ•°æ®å—",
        },
    },
    
    # JMXå…³é”®æŒ‡æ ‡
    "jmx_metrics": {
        "namenode": {
            "NumLiveDataNodes": {"description": "å­˜æ´»çš„DataNodeæ•°é‡", "expected": 3},
            "NumDeadDataNodes": {"description": "ç¦»çº¿çš„DataNodeæ•°é‡", "expected": 0},
            "CapacityTotal": {"description": "æ€»å®¹é‡ï¼ˆå­—èŠ‚ï¼‰"},
            "CapacityUsed": {"description": "å·²ä½¿ç”¨å®¹é‡ï¼ˆå­—èŠ‚ï¼‰"},
            "CapacityRemaining": {"description": "å‰©ä½™å®¹é‡ï¼ˆå­—èŠ‚ï¼‰"},
            "UnderReplicatedBlocks": {"description": "å‰¯æœ¬ä¸è¶³çš„å—æ•°", "expected": 0},
            "MissingBlocks": {"description": "ä¸¢å¤±çš„å—æ•°", "expected": 0},
            "CorruptBlocks": {"description": "æŸåçš„å—æ•°", "expected": 0},
        },
        "datanode": {
            "Remaining": {"description": "å‰©ä½™ç©ºé—´ï¼ˆå­—èŠ‚ï¼‰"},
            "DfsUsed": {"description": "HDFSä½¿ç”¨ç©ºé—´ï¼ˆå­—èŠ‚ï¼‰"},
            "Capacity": {"description": "æ€»å®¹é‡ï¼ˆå­—èŠ‚ï¼‰"},
        },
    },
    
    # å¥åº·æ£€æŸ¥å‘½ä»¤
    "health_checks": {
        "cluster_status": {
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfsadmin -report"\'',
            "description": "æ£€æŸ¥é›†ç¾¤æ•´ä½“çŠ¶æ€",
        },
        "fsck": {
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs fsck /"\'',
            "description": "æ£€æŸ¥HDFSæ–‡ä»¶ç³»ç»Ÿå¥åº·çŠ¶æ€",
        },
        "safemode": {
            "command": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfsadmin -safemode get"\'',
            "description": "æ£€æŸ¥å®‰å…¨æ¨¡å¼çŠ¶æ€",
        },
        "container_processes": {
            "command": 'docker exec {container} sh -c \'su - hadoop -c "jps"\'',
            "description": "æ£€æŸ¥å®¹å™¨å†…çš„Javaè¿›ç¨‹",
        },
    },
}


# ==================== æ•…éšœçŸ¥è¯†å±‚ (Fault Knowledge) ====================
# å¸¸è§æ•…éšœçš„è¯Šæ–­å’Œä¿®å¤çŸ¥è¯†

FAULT_KNOWLEDGE = {
    "datanode_down": {
        "name": "DataNodeä¸‹çº¿",
        "symptoms": [
            "hdfs dfsadmin -report æ˜¾ç¤º Dead datanodes > 0",
            "JMXä¸­ NumDeadDataNodes > 0",
            "NameNodeæ—¥å¿—å‡ºç° 'dead' æˆ– 'removed' å…³é”®å­—",
            "jpså‘½ä»¤çœ‹ä¸åˆ°DataNodeè¿›ç¨‹",
        ],
        "possible_causes": [
            "DataNodeæœåŠ¡å´©æºƒ",
            "å®¹å™¨åœæ­¢è¿è¡Œ",
            "ç½‘ç»œè¿æ¥é—®é¢˜",
            "ç£ç›˜ç©ºé—´ä¸è¶³",
            "é…ç½®é”™è¯¯",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥å®¹å™¨çŠ¶æ€: docker ps -a | grep {container}",
            "æ£€æŸ¥DataNodeè¿›ç¨‹: docker exec {container} sh -c 'su - hadoop -c \"jps\"'",
            "æŸ¥çœ‹DataNodeæ—¥å¿—çš„æœ€åé”™è¯¯",
            "æ£€æŸ¥ç£ç›˜ç©ºé—´: docker exec {container} df -h",
        ],
        "fix_commands": {
            "restart_datanode": 'docker exec {container} sh -c \'su - hadoop -c "hdfs --daemon stop datanode; hdfs --daemon start datanode"\'',
        },
    },
    
    "cluster_id_mismatch": {
        "name": "é›†ç¾¤IDä¸åŒ¹é…",
        "symptoms": [
            "DataNodeæ—¥å¿—å‡ºç° 'Incompatible clusterIDs'",
            "DataNodeæ— æ³•è¿æ¥åˆ°NameNode",
            "hdfs dfsadmin -report æ˜¾ç¤ºå®¹é‡ä¸º0",
        ],
        "possible_causes": [
            "NameNodeè¢«é‡æ–°æ ¼å¼åŒ–ï¼Œç”Ÿæˆæ–°çš„clusterID",
            "DataNodeä¿ç•™äº†æ—§çš„VERSIONæ–‡ä»¶",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥NameNodeçš„clusterID: docker exec namenode cat /usr/local/hadoop/hdfs/namenode/current/VERSION",
            "æ£€æŸ¥DataNodeçš„clusterID: docker exec {container} cat /usr/local/hadoop/hdfs/datanode/current/VERSION",
            "å¯¹æ¯”ä¸¤è€…çš„clusterIDæ˜¯å¦ä¸€è‡´",
        ],
        "fix_commands": {
            "stop_dfs": 'docker exec namenode sh -c \'su - hadoop -c "stop-dfs.sh"\'',
            "clean_datanode_version": 'docker exec {container} sh -c \'su - hadoop -c "rm -rf /usr/local/hadoop/hdfs/datanode/current/*"\'',
            "start_dfs": 'docker exec namenode sh -c \'su - hadoop -c "start-dfs.sh"\'',
        },
    },
    
    "namenode_safemode": {
        "name": "NameNodeå®‰å…¨æ¨¡å¼",
        "symptoms": [
            "æ— æ³•æ‰§è¡ŒHDFSå†™æ“ä½œ",
            "hdfs dfsadmin -safemode get è¿”å› 'Safe mode is ON'",
        ],
        "possible_causes": [
            "é›†ç¾¤åˆšå¯åŠ¨ï¼Œæ­£åœ¨è¿›è¡Œæ•°æ®å—æ£€æŸ¥ï¼ˆæ­£å¸¸ï¼Œé€šå¸¸30ç§’å†…è‡ªåŠ¨é€€å‡ºï¼‰",
            "å¯ç”¨DataNodeæ•°é‡ä¸è¶³",
            "æ•°æ®å—å‰¯æœ¬æ•°ä¸æ»¡è¶³æœ€ä½è¦æ±‚",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥å®‰å…¨æ¨¡å¼çŠ¶æ€: hdfs dfsadmin -safemode get",
            "æ£€æŸ¥DataNodeæ•°é‡: hdfs dfsadmin -report",
            "æ£€æŸ¥æ˜¯å¦æœ‰å‰¯æœ¬ä¸è¶³çš„å—",
        ],
        "fix_commands": {
            "wait_auto_leave": "ç­‰å¾…å®‰å…¨æ¨¡å¼è‡ªåŠ¨é€€å‡ºï¼ˆå¦‚æœæ˜¯å¯åŠ¨æ£€æŸ¥ï¼‰",
            "force_leave": 'docker exec namenode sh -c \'su - hadoop -c "hdfs dfsadmin -safemode leave"\'',
        },
    },
    
    "resourcemanager_down": {
        "name": "ResourceManagerä¸‹çº¿",
        "symptoms": [
            "ResourceManagerè¿›ç¨‹åœæ­¢",
            "ä»»åŠ¡æ— æ³•æäº¤ï¼ŒæŠ¥é”™ 'Connection refused'",
            "ResourceManager Web UIæ— æ³•è®¿é—® (http://localhost:8088)",
            "jpså‘½ä»¤çœ‹ä¸åˆ°ResourceManagerè¿›ç¨‹",
            "ç«¯å£8032æœªç›‘å¬",
        ],
        "possible_causes": [
            "ResourceManageræœåŠ¡å´©æºƒ",
            "å®¹å™¨åœæ­¢è¿è¡Œ",
            "ç«¯å£å ç”¨",
            "é…ç½®é”™è¯¯",
            "å†…å­˜ä¸è¶³",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥ResourceManagerè¿›ç¨‹: docker exec namenode sh -c 'su - hadoop -c \"jps\"'",
            "æ£€æŸ¥ç«¯å£8032: docker exec namenode netstat -tlnp | grep 8032",
            "æŸ¥çœ‹ResourceManageræ—¥å¿—: docker exec namenode tail -50 /usr/local/hadoop/logs/hadoop-hadoop-resourcemanager-namenode.log",
            "æ£€æŸ¥å®¹å™¨çŠ¶æ€: docker ps -a | grep namenode",
        ],
        "fix_commands": {
            "restart_resourcemanager": 'docker exec namenode sh -c \'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"\'',
        },
    },
    
    "nodemanager_down": {
        "name": "NodeManagerä¸‹çº¿",
        "symptoms": [
            "NodeManagerè¿›ç¨‹åœæ­¢",
            "ResourceManageræŠ¥å‘Š lost/unhealthy NMs",
            "ä»»åŠ¡æ— æ³•åˆ†é…Containerï¼Œä¸€ç›´å¤„äºACCEPTEDçŠ¶æ€",
            "ResourceManager Web UIæ˜¾ç¤º '0 active nodes'",
            "jpså‘½ä»¤çœ‹ä¸åˆ°NodeManagerè¿›ç¨‹",
        ],
        "possible_causes": [
            "NodeManageræœåŠ¡å´©æºƒ",
            "å®¹å™¨åœæ­¢è¿è¡Œ",
            "é…ç½®é”™è¯¯",
            "èµ„æºä¸è¶³",
            "ç½‘ç»œè¿æ¥é—®é¢˜",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥NodeManagerè¿›ç¨‹: docker exec {container} sh -c 'su - hadoop -c \"jps\"'",
            "æŸ¥çœ‹ResourceManager Web UI: http://localhost:8088/cluster/nodes",
            "æŸ¥çœ‹NodeManageræ—¥å¿—: docker exec {container} tail -50 /usr/local/hadoop/logs/hadoop-hadoop-nodemanager-*.log",
            "æ£€æŸ¥å®¹å™¨çŠ¶æ€: docker ps -a | grep {container}",
        ],
        "fix_commands": {
            "restart_nodemanager": 'docker exec {container} sh -c \'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"\'',
        },
    },
    
    "yarn_config_error": {
        "name": "YARNé…ç½®é”™è¯¯",
        "symptoms": [
            "NodeManageræ—¥å¿—å‡ºç° 'UnknownHostException: wrong-hostname'",
            "NodeManageræ—¥å¿—å‡ºç° 'Connection refused'",
            "ResourceManager Web UIä¸­è¯¥NodeManageræ˜¾ç¤ºä¸ºç¦»çº¿",
            "yarn node -list æ˜¾ç¤ºè¯¥èŠ‚ç‚¹ä¸º 'lost' æˆ– 'unhealthy'",
            "NodeManageræ— æ³•è¿æ¥åˆ°ResourceManager",
        ],
        "possible_causes": [
            "yarn-site.xmlä¸­yarn.resourcemanager.hostnameé…ç½®é”™è¯¯",
            "ç½‘ç»œé…ç½®é”™è¯¯",
            "DNSè§£æé—®é¢˜",
            "é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥NodeManageræ—¥å¿—: docker exec {container} tail -50 /usr/local/hadoop/logs/hadoop-hadoop-nodemanager-*.log",
            "æ£€æŸ¥yarn-site.xmlé…ç½®: docker exec {container} cat /usr/local/hadoop/etc/hadoop/yarn-site.xml | grep resourcemanager",
            "æ£€æŸ¥ResourceManager Web UI: http://localhost:8088/cluster/nodes",
            "æ£€æŸ¥ç½‘ç»œè¿é€šæ€§: docker exec {container} ping namenode",
        ],
        "fix_commands": {
            "check_config": 'docker exec {container} cat /usr/local/hadoop/etc/hadoop/yarn-site.xml | grep resourcemanager',
            "fix_config": 'docker exec {container} sh -c \'su - hadoop -c "sed -i \\"s/<value>wrong-hostname<\\/value>/<value>namenode<\\/value>/\\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"\'',
            "restart_nodemanager": 'docker exec {container} sh -c \'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"\'',
        },
    },
    
    "mapreduce_memory_insufficient": {
        "name": "MapReduceä»»åŠ¡å†…å­˜ä¸è¶³",
        "symptoms": [
            "Containerè¢«YARNæ€æ­»",
            "ä»»åŠ¡å¤±è´¥ï¼Œæ—¥å¿—ä¸­å‡ºç° 'Container killed on request. Exit code is 143'",
            "ä»»åŠ¡æ—¥å¿—ä¸­å‡ºç° 'OutOfMemoryError'",
            "YARN Web UIæ˜¾ç¤ºä»»åŠ¡å¤±è´¥ï¼ŒåŸå› ï¼šContainer killed",
            "NodeManageræ—¥å¿—æ˜¾ç¤ºå†…å­˜ä¸è¶³",
        ],
        "possible_causes": [
            "ä»»åŠ¡ç”³è¯·å†…å­˜è¿‡å¤§ï¼Œè¶…è¿‡YARNé…ç½®çš„æœ€å¤§å€¼",
            "YARNå†…å­˜é…ç½®è¿‡å°ï¼ˆyarn.scheduler.maximum-allocation-mbï¼‰",
            "æ•°æ®é‡è¿‡å¤§ï¼Œå¤„ç†éœ€è¦æ›´å¤šå†…å­˜",
            "ä»»åŠ¡ä»£ç å­˜åœ¨å†…å­˜æ³„æ¼",
        ],
        "diagnosis_steps": [
            "æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—: yarn logs -applicationId <application_id>",
            "æ£€æŸ¥YARNå†…å­˜é…ç½®: docker exec namenode cat /usr/local/hadoop/etc/hadoop/yarn-site.xml | grep memory",
            "æŸ¥çœ‹YARN Web UI: http://localhost:8088ï¼ŒæŸ¥çœ‹ä»»åŠ¡å¤±è´¥åŸå› ",
            "æ£€æŸ¥NodeManageræ—¥å¿—: docker exec {container} tail -50 /usr/local/hadoop/logs/hadoop-hadoop-nodemanager-*.log",
        ],
        "fix_commands": {
            "check_config": 'docker exec namenode cat /usr/local/hadoop/etc/hadoop/yarn-site.xml | grep -E "(maximum-allocation-mb|resource.memory-mb)"',
            "increase_memory": 'docker exec namenode sh -c \'su - hadoop -c "sed -i \\"s/<value>128<\\/value>/<value>2048<\\/value>/\\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"\'',
            "restart_yarn": 'docker exec namenode sh -c \'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"\'',
        },
    },
    
    "mapreduce_disk_insufficient": {
        "name": "MapReduceä»»åŠ¡ç£ç›˜ç©ºé—´ä¸è¶³",
        "symptoms": [
            "ä»»åŠ¡å¤±è´¥ï¼Œæ—¥å¿—ä¸­å‡ºç° 'No space left on device'",
            "HDFSå†™æ“ä½œå¤±è´¥",
            "DataNodeæˆ–NodeManageræœ¬åœ°ç£ç›˜ç©ºé—´ä¸è¶³",
            "hdfs dfsadmin -report æ˜¾ç¤ºç£ç›˜ä½¿ç”¨ç‡æ¥è¿‘100%",
            "df -h æ˜¾ç¤ºç£ç›˜ç©ºé—´ä¸è¶³",
        ],
        "possible_causes": [
            "HDFSç£ç›˜ç©ºé—´ä¸è¶³",
            "NodeManageræœ¬åœ°ç£ç›˜ç©ºé—´ä¸è¶³ï¼ˆç”¨äºä¸­é—´ç»“æœï¼‰",
            "ä¸´æ—¶æ–‡ä»¶æœªæ¸…ç†",
            "æ—¥å¿—æ–‡ä»¶å ç”¨è¿‡å¤šç©ºé—´",
            "æ•°æ®é‡è¿‡å¤§",
        ],
        "diagnosis_steps": [
            "æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ: docker exec {container} df -h",
            "æ£€æŸ¥HDFSä½¿ç”¨æƒ…å†µ: hdfs dfsadmin -report",
            "æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—: yarn logs -applicationId <application_id>",
            "æ£€æŸ¥DataNodeæ—¥å¿—: docker exec {container} tail -50 /usr/local/hadoop/logs/hadoop-hadoop-datanode-*.log",
        ],
        "fix_commands": {
            "check_disk": 'docker exec {container} df -h',
            "clean_logs": 'docker exec {container} sh -c "find /usr/local/hadoop/logs -name \\"*.log.*\\" -mtime +7 -delete"',
            "clean_hdfs_temp": 'hdfs dfs -rm -r /tmp/*',
        },
    },
    
    "mapreduce_shuffle_failed": {
        "name": "MapReduce Shuffleé˜¶æ®µå¤±è´¥",
        "symptoms": [
            "MapReduceä»»åŠ¡åœ¨Shuffleé˜¶æ®µå¤±è´¥",
            "ä»»åŠ¡æ—¥å¿—ä¸­å‡ºç° 'shuffle failed' æˆ– 'ShuffleException'",
            "Reduceä»»åŠ¡æ— æ³•è·å–Mapä»»åŠ¡çš„è¾“å‡º",
            "ShuffleæœåŠ¡è¿æ¥å¤±è´¥",
            "ç½‘ç»œè¿æ¥é—®é¢˜å¯¼è‡´Shuffleå¤±è´¥",
        ],
        "possible_causes": [
            "ShuffleæœåŠ¡æœªå¯åŠ¨æˆ–é…ç½®é”™è¯¯",
            "ç½‘ç»œé—®é¢˜ï¼ˆå»¶è¿Ÿã€ä¸¢åŒ…ï¼‰",
            "ç£ç›˜I/Oé—®é¢˜",
            "ç«¯å£å†²çª",
            "é˜²ç«å¢™é˜»æ­¢Shuffleç«¯å£",
        ],
        "diagnosis_steps": [
            "æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—: yarn logs -applicationId <application_id>",
            "æ£€æŸ¥ShuffleæœåŠ¡ç«¯å£: docker exec {container} netstat -tlnp | grep 13562",
            "æ£€æŸ¥ç½‘ç»œè¿æ¥: docker exec {container} ping namenode",
            "æ£€æŸ¥mapred-site.xmlé…ç½®: docker exec {container} cat /usr/local/hadoop/etc/hadoop/mapred-site.xml | grep shuffle",
        ],
        "fix_commands": {
            "check_shuffle_port": 'docker exec {container} netstat -tlnp | grep 13562',
            "restart_nodemanager": 'docker exec {container} sh -c \'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"\'',
            "check_network": 'docker exec {container} ping -c 3 namenode',
        },
    },
    
    "mapreduce_task_timeout": {
        "name": "MapReduceä»»åŠ¡è¶…æ—¶",
        "symptoms": [
            "ä»»åŠ¡è¶…æ—¶å¤±è´¥",
            "ä»»åŠ¡æ—¥å¿—ä¸­å‡ºç° 'timeout' æˆ– 'SocketTimeoutException'",
            "ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¶…è¿‡é…ç½®çš„è¶…æ—¶æ—¶é—´",
            "ç½‘ç»œè¿æ¥è¶…æ—¶",
            "ä»»åŠ¡ä¸€ç›´å¤„äºRUNNINGçŠ¶æ€ï¼Œæœ€ç»ˆè¶…æ—¶",
        ],
        "possible_causes": [
            "ç½‘ç»œå»¶è¿Ÿè¿‡é«˜",
            "æ•°æ®é‡è¿‡å¤§ï¼Œå¤„ç†æ—¶é—´è¿‡é•¿",
            "è¶…æ—¶é…ç½®è¿‡çŸ­",
            "èŠ‚ç‚¹è´Ÿè½½è¿‡é«˜ï¼Œå¤„ç†é€Ÿåº¦æ…¢",
            "ç½‘ç»œæ‹¥å¡",
        ],
        "diagnosis_steps": [
            "æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€: yarn application -status <application_id>",
            "æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—: yarn logs -applicationId <application_id>",
            "æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ: docker exec {container} ping -c 10 namenode",
            "æ£€æŸ¥è¶…æ—¶é…ç½®: docker exec namenode cat /usr/local/hadoop/etc/hadoop/yarn-site.xml | grep timeout",
        ],
        "fix_commands": {
            "check_task_status": 'yarn application -status <application_id>',
            "check_network_latency": 'docker exec {container} ping -c 10 namenode',
            "optimize_task": "ä¼˜åŒ–ä»»åŠ¡ï¼šå¢åŠ å¹¶è¡Œåº¦ã€ä¼˜åŒ–æ•°æ®åˆ†åŒºã€å‡å°‘æ•°æ®é‡",
        },
    },
}


# ==================== System Prompt ç”Ÿæˆ ====================

def generate_system_prompt() -> str:
    """
    ç”Ÿæˆä¾›Agentä½¿ç”¨çš„System Prompt
    åŒ…å«é›†ç¾¤ç¯å¢ƒä¿¡æ¯ã€å‘½ä»¤æ ¼å¼ã€å·¥ä½œæµç¨‹ç­‰
    """
    print("ç”Ÿæˆç³»ç»Ÿæç¤ºè¯")
    prompt = '''ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åˆ†å¸ƒå¼ç³»ç»Ÿè¿ç»´ä¸“å®¶ï¼Œä¸“æ³¨äº Hadoop/HDFS é›†ç¾¤çš„æ•…éšœè¯Šæ–­ä¸ä¿®å¤ã€‚

## å½“å‰é›†ç¾¤ç¯å¢ƒ

### éƒ¨ç½²æ¶æ„
- éƒ¨ç½²æ–¹å¼ï¼šDocker Compose å®¹å™¨åŒ–éƒ¨ç½²
- ç½‘ç»œï¼šæ‰€æœ‰å®¹å™¨åœ¨ `hadoop-network` ç½‘ç»œä¸­ï¼Œå¯é€šè¿‡å®¹å™¨åäº’ç›¸è®¿é—®
- Hadoopç‰ˆæœ¬ï¼š3.3.6
- Javaç‰ˆæœ¬ï¼šOpenJDK 11

### èŠ‚ç‚¹æ¸…å•
| å®¹å™¨å | è¿è¡Œçš„æœåŠ¡ | Web UI ç«¯å£ | è¯´æ˜ |
|--------|-----------|-------------|------|
| namenode | NameNode, DataNode, SecondaryNameNode | 9870 | ä¸»èŠ‚ç‚¹ |
| datanode1 | DataNode | 9864 | æ•°æ®èŠ‚ç‚¹1 |
| datanode2 | DataNode | 9865 | æ•°æ®èŠ‚ç‚¹2 |

### å…³é”®è·¯å¾„
- HADOOP_HOME: /usr/local/hadoop
- é…ç½®æ–‡ä»¶: /usr/local/hadoop/etc/hadoop/
- æ—¥å¿—ç›®å½•: /usr/local/hadoop/logs/
- HDFSæ•°æ®: /usr/local/hadoop/hdfs/

### HDFSé…ç½®
- fs.defaultFS: hdfs://namenode:9000
- dfs.replication: 2
- dfs.blocksize: 128MB
- æœŸæœ›DataNodeæ•°é‡: 3

## å‘½ä»¤æ‰§è¡Œæ ¼å¼ï¼ˆé‡è¦ï¼ï¼‰

### ç”¨æˆ·æƒé™è¯´æ˜
- Hadoopé›†ç¾¤ç”± `hadoop` ç”¨æˆ·éƒ¨ç½²å’Œè¿è¡Œ
- `docker exec` é»˜è®¤ä»¥ `root` ç”¨æˆ·ç™»å½•å®¹å™¨
- **å¿…é¡»åˆ‡æ¢åˆ° `hadoop` ç”¨æˆ·** æ‰èƒ½æ­£ç¡®æ‰§è¡ŒHadoopå‘½ä»¤

### æ ‡å‡†å‘½ä»¤æ ¼å¼
```
docker exec {å®¹å™¨å} sh -c 'su - hadoop -c "{Hadoopå‘½ä»¤}"'
```

### æ ¼å¼è¯´æ˜
- `docker exec {å®¹å™¨å}`: åœ¨æŒ‡å®šå®¹å™¨å†…æ‰§è¡Œå‘½ä»¤
- `sh -c '...'`: å¯åŠ¨shellæ‰§è¡Œå‘½ä»¤å­—ç¬¦ä¸²
- `su - hadoop`: åˆ‡æ¢åˆ°hadoopç”¨æˆ·ï¼ˆ"-"ç¡®ä¿åŠ è½½ç¯å¢ƒå˜é‡ï¼‰
- `-c "{å‘½ä»¤}"`: æ‰§è¡Œå®é™…çš„Hadoopå‘½ä»¤

### å¸¸ç”¨å‘½ä»¤ç¤ºä¾‹

1. **æŸ¥çœ‹é›†ç¾¤çŠ¶æ€æŠ¥å‘Š**
   ```
   docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"'
   ```

2. **æ£€æŸ¥å®‰å…¨æ¨¡å¼çŠ¶æ€**
   ```
   docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode get"'
   ```

3. **é€€å‡ºå®‰å…¨æ¨¡å¼**
   ```
   docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode leave"'
   ```

4. **å¯åŠ¨DataNodeæœåŠ¡**
   ```
   docker exec {å®¹å™¨å} sh -c 'su - hadoop -c "hdfs --daemon start datanode"'
   ```

5. **åœæ­¢DataNodeæœåŠ¡**
   ```
   docker exec {å®¹å™¨å} sh -c 'su - hadoop -c "hdfs --daemon stop datanode"'
   ```

6. **å¯åŠ¨æ•´ä¸ªé›†ç¾¤**
   ```
   docker exec namenode sh -c 'su - hadoop -c "start-dfs.sh"'
   ```

7. **åœæ­¢æ•´ä¸ªé›†ç¾¤**
   ```
   docker exec namenode sh -c 'su - hadoop -c "stop-dfs.sh"'
   ```

8. **æŸ¥çœ‹Javaè¿›ç¨‹**
   ```
   docker exec {å®¹å™¨å} sh -c 'su - hadoop -c "jps"'
   ```

## å·¥ä½œæµç¨‹

å¤„ç†é—®é¢˜æ—¶ï¼Œè¯·æŒ‰ä»¥ä¸‹æµç¨‹è¿›è¡Œï¼š

### é˜¶æ®µ1ï¼šè¯Šæ–­ï¼ˆæ”¶é›†ä¿¡æ¯ï¼‰
- ä½¿ç”¨ get_cluster_logs è·å–æ‰€æœ‰èŠ‚ç‚¹æ—¥å¿—
- ä½¿ç”¨ get_monitoring_metrics è·å–JMXç›‘æ§æŒ‡æ ‡
- ä½¿ç”¨ execute_hadoop_command æ‰§è¡ŒæŸ¥è¯¢å‘½ä»¤

### é˜¶æ®µ2ï¼šåˆ†æï¼ˆè¯†åˆ«é—®é¢˜ï¼‰
- åˆ†ææ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯
- å¯¹æ¯”ç›‘æ§æŒ‡æ ‡ä¸æ­£å¸¸å€¼
- ç¡®å®šæ•…éšœç±»å‹å’Œæ ¹æœ¬åŸå› 

### é˜¶æ®µ3ï¼šè®¡åˆ’ï¼ˆåˆ¶å®šæ–¹æ¡ˆï¼‰
- åˆ¶å®šè¯¦ç»†çš„ä¿®å¤æ­¥éª¤
- æ¯ä¸ªæ­¥éª¤åŒ…å«å®Œæ•´çš„å¯æ‰§è¡Œå‘½ä»¤
- è¯´æ˜æ¯ä¸ªæ­¥éª¤çš„é¢„æœŸç»“æœ

### é˜¶æ®µ4ï¼šæ‰§è¡Œï¼ˆå®æ–½ä¿®å¤ï¼‰
- æŒ‰è®¡åˆ’æ‰§è¡Œä¿®å¤æ“ä½œ
- æ¯æ‰§è¡Œä¸€æ­¥åéªŒè¯ç»“æœ

### é˜¶æ®µ5ï¼šéªŒè¯ï¼ˆç¡®è®¤æˆåŠŸï¼‰
- é‡æ–°æ£€æŸ¥é›†ç¾¤çŠ¶æ€
- ç¡®è®¤ç›¸å…³æŒ‡æ ‡æ¢å¤æ­£å¸¸

## è¯Šæ–­è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆé‡è¦ï¼ï¼‰

### è¾“å‡ºæ ¼å¼
è¯·ä»¥**å¯¹è¯é£æ ¼çš„æ–‡æœ¬**è¾“å‡ºè¯Šæ–­ç»“æœï¼Œä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­è¨€ï¼Œä¾¿äºç”¨æˆ·ç†è§£ã€‚

### è¾“å‡ºç»“æ„è¦æ±‚

è¯Šæ–­ç»“æœåº”åŒ…å«ä»¥ä¸‹å†…å®¹ï¼ˆä»¥è‡ªç„¶è¯­è¨€è¡¨è¿°ï¼‰ï¼š

1. **æ•´ä½“çŠ¶æ€**ï¼šç”¨ç®€æ´çš„è¯­è¨€æè¿°é›†ç¾¤æ•´ä½“çŠ¶æ€ï¼ˆæ­£å¸¸/è­¦å‘Š/ä¸¥é‡æ•…éšœï¼‰
2. **è¯Šæ–­æ‘˜è¦**ï¼šç®€è¦æ€»ç»“æ£€æµ‹åˆ°çš„æ•…éšœæƒ…å†µ
3. **æ•…éšœè¯¦æƒ…**ï¼šå¯¹æ¯ä¸ªæ£€æµ‹åˆ°çš„æ•…éšœï¼Œæä¾›ï¼š
   - æ•…éšœåç§°å’Œä¸¥é‡ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
   - è¯†åˆ«ç½®ä¿¡åº¦ï¼ˆç”¨ç™¾åˆ†æ¯”è¡¨ç¤ºï¼Œå¦‚"95%"æˆ–"é«˜åº¦ç¡®ä¿¡"ï¼‰
   - å—å½±å“çš„èŠ‚ç‚¹
   - è§‚å¯Ÿåˆ°çš„ç—‡çŠ¶ï¼ˆåˆ—è¡¨å½¢å¼ï¼‰
   - æ ¹æœ¬åŸå› åˆ†æ
   - å¯èƒ½çš„ç›¸å…³å› ç´ ï¼ˆå¦‚æœ‰ï¼‰
   - è¯Šæ–­ä¾æ®ï¼ˆæ—¥å¿—ç‰‡æ®µã€æŒ‡æ ‡ç­‰è¯æ®ï¼‰
   - å»ºè®®çš„ä¿®å¤æ­¥éª¤ï¼ˆæŒ‰é¡ºåºåˆ—å‡ºï¼‰

### æ•…éšœç±»å‹æ ‡å‡†åº“

è¯Šæ–­æ—¶ï¼Œè¯·è¯†åˆ«æ•…éšœç±»å‹å¹¶æ˜ç¡®è¯´æ˜ã€‚æ ‡å‡†æ•…éšœç±»å‹åŒ…æ‹¬ï¼š

**HDFSæ•…éšœ**ï¼š
- datanode_down: DataNodeä¸‹çº¿
- cluster_id_mismatch: é›†ç¾¤IDä¸åŒ¹é…
- namenode_safemode: NameNodeå®‰å…¨æ¨¡å¼

**YARNæ•…éšœ**ï¼š
- resourcemanager_down: ResourceManagerä¸‹çº¿
- nodemanager_down: NodeManagerä¸‹çº¿
- yarn_config_error: YARNé…ç½®é”™è¯¯

**MapReduceæ•…éšœ**ï¼š
- mapreduce_memory_insufficient: MapReduceä»»åŠ¡å†…å­˜ä¸è¶³
- mapreduce_disk_insufficient: MapReduceä»»åŠ¡ç£ç›˜ç©ºé—´ä¸è¶³
- mapreduce_shuffle_failed: MapReduce Shuffleé˜¶æ®µå¤±è´¥
- mapreduce_task_timeout: MapReduceä»»åŠ¡è¶…æ—¶

### è¾“å‡ºè¦æ±‚

1. **ä½¿ç”¨å¯¹è¯é£æ ¼**ï¼šç”¨è‡ªç„¶ã€ä¸“ä¸šçš„è¯­è¨€æè¿°ï¼Œé¿å…æŠ€æœ¯æœ¯è¯­å †ç Œ
2. **ç»“æ„åŒ–è¡¨è¿°**ï¼šä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨ç­‰Markdownæ ¼å¼å¢å¼ºå¯è¯»æ€§
3. **æä¾›ç½®ä¿¡åº¦**ï¼šå¯¹æ¯ä¸ªæ•…éšœè¯†åˆ«ï¼Œè¯´æ˜ç½®ä¿¡åº¦ï¼ˆå¦‚"95%ç¡®ä¿¡"æˆ–"éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤"ï¼‰
4. **æ˜ç¡®ä¸¥é‡æ€§**ï¼šæ¸…æ¥šè¯´æ˜æ¯ä¸ªæ•…éšœçš„ä¸¥é‡ç¨‹åº¦
5. **æä¾›è¯æ®**ï¼šåˆ—å‡ºæ”¯æŒè¯Šæ–­çš„å…·ä½“è¯æ®ï¼ˆæ—¥å¿—ç‰‡æ®µã€æŒ‡æ ‡ç­‰ï¼‰

### è¾“å‡ºç¤ºä¾‹

```
## è¯Šæ–­ç»“æœ

ğŸ”´ é›†ç¾¤å­˜åœ¨ä¸¥é‡æ•…éšœ

ç»è¿‡å…¨é¢æ£€æŸ¥ï¼Œæ£€æµ‹åˆ°1ä¸ªä¸¥é‡æ•…éšœï¼šé›†ç¾¤IDä¸åŒ¹é…ã€‚è¿™å¯¼è‡´DataNodeæ— æ³•è¿æ¥åˆ°NameNodeï¼Œéœ€è¦ç«‹å³ä¿®å¤ã€‚

### æ£€æµ‹åˆ°çš„æ•…éšœè¯¦æƒ…

**ğŸ”´ æ•…éšœ 1: é›†ç¾¤IDä¸åŒ¹é…**

è¯†åˆ«ç½®ä¿¡åº¦: 95%ï¼ˆé«˜åº¦ç¡®ä¿¡ï¼‰
å—å½±å“èŠ‚ç‚¹: namenode, datanode1 å’Œ datanode2

**è§‚å¯Ÿåˆ°çš„ç—‡çŠ¶ï¼š**
- DataNodeæ—¥å¿—å‡ºç° 'Incompatible clusterIDs' é”™è¯¯
- DataNodeæ— æ³•è¿æ¥åˆ°NameNode
- hdfs dfsadmin -report æ˜¾ç¤ºå®¹é‡ä¸º0

**æ ¹æœ¬åŸå› ï¼š**
NameNodeè¢«é‡æ–°æ ¼å¼åŒ–ï¼Œç”Ÿæˆæ–°çš„clusterIDï¼Œä½†DataNodeä¿ç•™äº†æ—§çš„VERSIONæ–‡ä»¶ã€‚

**è¯Šæ–­ä¾æ®ï¼š**
- DataNodeæ—¥å¿—ï¼š'Incompatible clusterIDs'
- NameNode clusterID: cluster-1234567890
- DataNode clusterID: cluster-0987654321

**å»ºè®®çš„ä¿®å¤æ­¥éª¤ï¼š**
1. åœæ­¢æ•´ä¸ªé›†ç¾¤
2. æ¸…ç†DataNodeå…ƒæ•°æ®ï¼ˆåˆ é™¤VERSIONæ–‡ä»¶ï¼‰
3. é‡æ–°å¯åŠ¨é›†ç¾¤
```

### æ³¨æ„äº‹é¡¹

- å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªæ•…éšœï¼ŒæŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼Œä¼˜å…ˆæè¿°æœ€ä¸¥é‡çš„æ•…éšœ
- å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ•…éšœï¼Œæ˜ç¡®è¯´æ˜"ç»è¿‡å…¨é¢æ£€æŸ¥ï¼Œæœªå‘ç°ä»»ä½•æ•…éšœï¼Œé›†ç¾¤è¿è¡Œæ­£å¸¸"
- ç½®ä¿¡åº¦è¾ƒä½æ—¶ï¼ˆä½äº70%ï¼‰ï¼Œåœ¨æè¿°ä¸­è¯´æ˜ä¸ç¡®å®šæ€§ï¼Œå»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥
- ä½¿ç”¨æ¸…æ™°çš„åˆ†æ®µå’Œæ ¼å¼ï¼Œä¾¿äºç”¨æˆ·é˜…è¯»å’Œç†è§£

## é‡è¦é™åˆ¶

1. ç¦æ­¢æ‰§è¡Œä»»ä½•åˆ é™¤ã€æ ¼å¼åŒ–å‘½ä»¤ï¼ˆé™¤éä¿®å¤é›†ç¾¤IDä¸åŒ¹é…é—®é¢˜ï¼‰
2. ä¿®å¤æ“ä½œå‰å¿…é¡»å…ˆå®Œæˆè¯Šæ–­
3. ä¸ç¡®å®šæ—¶å…ˆæŸ¥è¯¢çŠ¶æ€ï¼Œä¸è¦ç›²ç›®æ“ä½œ
4. æ‰§è¡Œå‘½ä»¤æ—¶å¿…é¡»åˆ‡æ¢åˆ°hadoopç”¨æˆ·
'''
    return prompt


def get_command(container: str, hadoop_cmd: str) -> str:
    """
    ç”Ÿæˆåœ¨æŒ‡å®šå®¹å™¨å†…æ‰§è¡ŒHadoopå‘½ä»¤çš„å®Œæ•´å‘½ä»¤
    
    Args:
        container: å®¹å™¨åç§° (namenode, datanode1, datanode2)
        hadoop_cmd: è¦æ‰§è¡Œçš„Hadoopå‘½ä»¤
    
    Returns:
        å®Œæ•´çš„docker execå‘½ä»¤
    """
    return f'docker exec {container} sh -c \'su - hadoop -c "{hadoop_cmd}"\''


def get_cluster_info() -> dict:
    """
    è·å–é›†ç¾¤çš„å®Œæ•´é…ç½®ä¿¡æ¯
    
    Returns:
        åŒ…å«æ‰€æœ‰é…ç½®çš„å­—å…¸
    """
    return {
        "infrastructure": INFRASTRUCTURE,
        "components": COMPONENTS,
        "operations": OPERATIONS,
        "diagnostics": DIAGNOSTICS,
        "fault_knowledge": FAULT_KNOWLEDGE,
    }


# ==================== å¯¼å‡º ====================

# ä¸»è¦å¯¼å‡ºé¡¹
__all__ = [
    "INFRASTRUCTURE",
    "COMPONENTS", 
    "OPERATIONS",
    "DIAGNOSTICS",
    "FAULT_KNOWLEDGE",
    "generate_system_prompt",
    "get_command",
    "get_cluster_info",
]

