# 日志过滤白名单说明

## 问题背景

在将日志输入大模型前，系统会过滤掉 INFO 级别的日志，导致重要的 INFO 日志（如 `removeDeadDatanode`）也被过滤掉，无法被大模型分析。

## 解决方案

已修改 `cl_agent/log_reader.py` 中的 `should_filter_log_line` 函数，添加了**重要关键词白名单机制**。

### 工作原理

1. **默认行为**：仍然过滤 INFO 级别的日志（减少噪音）
2. **白名单机制**：如果 INFO 日志包含重要关键词，则**保留该日志**
3. **智能过滤**：既减少了日志量，又保留了关键信息

### 白名单关键词列表

以下关键词的 INFO 日志会被保留：

#### DataNode 相关
- `removeDeadDatanode` - DataNode 下线
- `lost heartbeat` - 心跳丢失
- `registerDatanode` - DataNode 注册
- `dead.*datanode` - 死掉的 DataNode
- `removed.*datanode` - 移除的 DataNode

#### 安全模式相关
- `safe mode` - 安全模式
- `Leaving safe mode` - 退出安全模式
- `Entering safe mode` - 进入安全模式

#### 块相关
- `UnderReplicatedBlocks` - 副本不足的块
- `MissingBlocks` - 丢失的块
- `CorruptBlocks` - 损坏的块

#### 网络拓扑相关
- `Removing a node` - 移除节点
- `Adding a new node` - 添加节点

#### 集群ID相关
- `Incompatible clusterIDs` - 集群ID不匹配
- `clusterID` - 集群ID相关

#### 连接相关
- `Connection refused` - 连接被拒绝
- `connection.*failed` - 连接失败

#### 磁盘空间相关
- `No space left` - 磁盘空间不足
- `disk.*full` - 磁盘满

## 代码实现

```python
# 过滤INFO级别日志
if filter_info:
    # 检查是否是INFO级别日志
    is_info_log = re.search(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}[,\s]+\d+\s+INFO\s+', line)
    
    if is_info_log:
        # 定义重要的INFO日志关键词白名单
        important_keywords = [
            r'removeDeadDatanode',      # DataNode下线
            r'lost heartbeat',          # 心跳丢失
            # ... 其他关键词
        ]
        
        # 如果包含重要关键词，保留该日志
        for keyword in important_keywords:
            if re.search(keyword, line, re.IGNORECASE):
                return False  # 保留该行
        
        # 不包含重要关键词的INFO日志，过滤掉
        return True
```

## 效果

### 修改前
```
2026-01-26 07:20:58,645 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 192.168.96.4:9866
```
**结果：** 被过滤掉，大模型看不到

### 修改后
```
2026-01-26 07:20:58,645 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* removeDeadDatanode: lost heartbeat from 192.168.96.4:9866
```
**结果：** 被保留，大模型可以看到并分析

## 如何添加新的关键词

如果需要添加新的重要关键词，编辑 `cl_agent/log_reader.py` 文件中的 `should_filter_log_line` 函数，在 `important_keywords` 列表中添加新的正则表达式：

```python
important_keywords = [
    # ... 现有关键词
    r'your_new_keyword',  # 你的新关键词
]
```

**注意：**
- 使用正则表达式格式（`r'...'`）
- 关键词不区分大小写（使用 `re.IGNORECASE`）
- 可以使用正则表达式的特殊字符（如 `.*` 表示任意字符）

## 验证

可以通过以下方式验证过滤是否正常工作：

```bash
# 1. 查看过滤后的日志
# 在 Gradio 界面中请求"查看集群日志，分析集群状态"

# 2. 检查日志文件
# 查看 result/cluster_logs_*.txt 文件，应该能看到 removeDeadDatanode 日志

# 3. 测试特定关键词
# 在日志中搜索关键词，确认是否被保留
```

## 配置选项

在 `cl_agent/config.py` 中：

```python
FILTER_INFO_LOGS = True      # 是否过滤INFO级别日志（启用白名单机制）
FILTER_CLASSPATH_LOGS = True  # 是否过滤classpath行
```

**建议：**
- 保持 `FILTER_INFO_LOGS = True`，使用白名单机制
- 如果完全不过滤 INFO 日志，可以设置 `FILTER_INFO_LOGS = False`（但会产生大量日志）

---

## 总结

✅ **问题已解决**：重要的 INFO 日志（如 `removeDeadDatanode`）现在会被保留  
✅ **智能过滤**：既减少了日志噪音，又保留了关键信息  
✅ **可扩展**：可以轻松添加新的关键词到白名单
