# 知识库文档导入指南

## 一、功能说明

`knowledge_base.py` 现在支持将完整的文档文件导入到知识库中，而不仅仅是代码中写死的少量内容。

### 支持的文档格式

- **文本文件**: `.txt`
- **Markdown文件**: `.md`, `.markdown`
- **PDF文件**: `.pdf`
- **其他格式**: 尝试使用通用加载器

### 核心功能

1. **文档加载** (`load_document`): 自动识别文件格式并加载
2. **文本分割** (`split_documents`): 将长文档分割成适合检索的块
3. **批量导入** (`import_document_to_kb`): 将单个文档导入知识库
4. **目录导入** (`import_directory_to_kb`): 批量导入目录中的所有文档

---

## 二、Hadoop 文档资源

### 2.1 官方文档

#### Apache Hadoop 官方文档
- **英文版**: https://hadoop.apache.org/docs/stable/
- **中文版**: https://hadoop.apache.org/docs/r1.0.4/cn/index.html
- **GitHub源码**: https://github.com/apache/hadoop
  - 文档位置: `hadoop-project-dist/hadoop-common/src/site/markdown/`

#### 下载方式

**方式1: 从GitHub克隆**
```bash
git clone https://github.com/apache/hadoop.git
cd hadoop
# 文档在 hadoop-project-dist/hadoop-common/src/site/markdown/ 目录下
```

**方式2: 从官网下载**
- 访问 https://hadoop.apache.org/docs/stable/
- 使用浏览器插件（如 SingleFile）保存为Markdown或HTML
- 或使用爬虫工具下载整个文档站点

**方式3: 使用wget下载**
```bash
# 下载Hadoop文档站点（需要配置）
wget -r -np -k https://hadoop.apache.org/docs/stable/
```

### 2.2 推荐的Hadoop文档内容

1. **HDFS用户指南**
   - HDFS架构
   - 配置说明
   - 故障排查

2. **YARN文档**
   - YARN架构
   - 资源管理
   - 应用开发

3. **MapReduce文档**
   - MapReduce编程模型
   - 性能调优

4. **集群管理文档**
   - 集群部署
   - 监控和运维
   - 安全配置

### 2.3 中文Hadoop资源

- **Hadoop中文文档**: https://hadoop.apache.org/docs/r1.0.4/cn/
- **Hadoop权威指南（中文版）**: 可以找到PDF版本
- **CSDN/博客园等**: 收集高质量的Hadoop故障排查文章

---

## 三、Docker 文档资源

### 3.1 官方文档

#### Docker 官方文档
- **英文版**: https://docs.docker.com/
- **中文版**: https://docs.docker.com/zh-cn/
- **GitHub源码**: https://github.com/docker/docs
  - 文档位置: `content/` 目录（Markdown格式）

#### 下载方式

**方式1: 从GitHub克隆**
```bash
git clone https://github.com/docker/docs.git
cd docs
# 文档在 content/ 目录下，都是Markdown格式
```

**方式2: 从官网下载**
- 访问 https://docs.docker.com/
- 使用浏览器插件保存
- 或使用爬虫工具下载

**方式3: 使用Docker官方文档生成工具**
```bash
# Docker文档是开源的，可以直接克隆
git clone https://github.com/docker/docs.git
```

### 3.2 推荐的Docker文档内容

1. **Docker基础**
   - 安装和配置
   - 镜像和容器
   - Dockerfile编写

2. **Docker Compose**
   - Compose文件格式
   - 多容器应用管理

3. **Docker网络**
   - 网络配置
   - 容器间通信

4. **Docker存储**
   - 数据卷管理
   - 存储驱动

5. **Docker故障排查**
   - 常见问题
   - 日志分析
   - 性能调优

---

## 四、使用示例

### 4.1 导入单个文档

```python
from lc_agent.knowledge_base import import_document_to_kb

# 导入Hadoop文档到HadoopDocs知识库
result = import_document_to_kb(
    file_path="/path/to/hadoop-hdfs-guide.md",
    kb_name="HadoopDocs",
    chunk_size=500,      # 每个块500字符
    chunk_overlap=50,    # 重叠50字符
    encoding="utf-8",
    metadata={
        "source": "Hadoop官方文档",
        "category": "HDFS",
        "version": "3.3.0"
    }
)

print(result)
# 输出: {'success': True, 'kb_name': 'HadoopDocs', 'file_name': 'hadoop-hdfs-guide.md', 'total_chunks': 45, 'message': '成功导入 45 个文档块'}
```

### 4.2 批量导入目录

```python
from lc_agent.knowledge_base import import_directory_to_kb

# 导入整个目录的文档
result = import_directory_to_kb(
    directory_path="/path/to/hadoop-docs",
    kb_name="HadoopDocs",
    chunk_size=500,
    chunk_overlap=50,
    file_extensions=['.md', '.txt', '.pdf'],  # 只导入这些格式
    metadata={
        "source": "Hadoop官方文档",
        "version": "3.3.0"
    }
)

print(f"成功导入: {result['success_files']}/{result['total_files']} 个文件")
if result['failed_files']:
    print(f"失败文件: {result['failed_files']}")
```

### 4.3 导入Docker文档

```python
# 导入Docker文档
result = import_directory_to_kb(
    directory_path="/path/to/docker-docs/content",
    kb_name="DockerDocs",
    chunk_size=500,
    chunk_overlap=50,
    metadata={
        "source": "Docker官方文档",
        "category": "Docker"
    }
)
```

### 4.4 创建新的知识库并导入

```python
from lc_agent.knowledge_base import get_kb_manager, import_document_to_kb

# 创建新的知识库
kb_manager = get_kb_manager()
docker_kb = kb_manager.get_or_create_kb("DockerExpert")

# 导入文档
result = import_document_to_kb(
    file_path="/path/to/docker-troubleshooting.md",
    kb_name="DockerExpert",
    chunk_size=500,
    chunk_overlap=50
)
```

---

## 五、完整工作流程

### 5.1 准备文档

```bash
# 1. 克隆Hadoop文档
git clone https://github.com/apache/hadoop.git
cd hadoop/hadoop-project-dist/hadoop-common/src/site/markdown/

# 2. 克隆Docker文档
git clone https://github.com/docker/docs.git
cd docs/content/
```

### 5.2 导入脚本示例

创建 `import_docs.py`:

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
批量导入文档到知识库
"""

import logging
from lc_agent.knowledge_base import (
    import_directory_to_kb,
    get_kb_manager
)

logging.basicConfig(level=logging.INFO)

def main():
    # 导入Hadoop文档
    print("=" * 50)
    print("导入Hadoop文档...")
    print("=" * 50)
    
    hadoop_result = import_directory_to_kb(
        directory_path="/path/to/hadoop-docs",
        kb_name="HadoopDocs",
        chunk_size=500,
        chunk_overlap=50,
        metadata={
            "source": "Hadoop官方文档",
            "version": "3.3.0"
        }
    )
    
    print(f"Hadoop文档导入结果: {hadoop_result['message']}")
    print(f"成功: {hadoop_result['success_files']}/{hadoop_result['total_files']}")
    
    # 导入Docker文档
    print("\n" + "=" * 50)
    print("导入Docker文档...")
    print("=" * 50)
    
    docker_result = import_directory_to_kb(
        directory_path="/path/to/docker-docs",
        kb_name="DockerDocs",
        chunk_size=500,
        chunk_overlap=50,
        metadata={
            "source": "Docker官方文档"
        }
    )
    
    print(f"Docker文档导入结果: {docker_result['message']}")
    print(f"成功: {docker_result['success_files']}/{docker_result['total_files']}")
    
    # 保存所有知识库
    kb_manager = get_kb_manager()
    for kb in kb_manager.knowledge_bases.values():
        kb.save()
        print(f"已保存知识库: {kb.kb_name}")

if __name__ == "__main__":
    main()
```

### 5.3 运行导入

```bash
python import_docs.py
```

---

## 六、参数说明

### 6.1 chunk_size（文本块大小）

- **推荐值**: 300-1000 字符
- **说明**: 
  - 太小：可能丢失上下文
  - 太大：检索精度下降
  - 中文文档建议 500-800
  - 英文文档建议 800-1000

### 6.2 chunk_overlap（重叠大小）

- **推荐值**: 50-200 字符
- **说明**: 
  - 保持上下文连续性
  - 避免重要信息被分割
  - 通常设置为 chunk_size 的 10-20%

### 6.3 知识库选择

根据文档内容选择合适的知识库：

- **HadoopDocs**: Hadoop官方文档
- **NameNodeExpert**: NameNode相关故障案例
- **DataNodeExpert**: DataNode相关故障案例
- **YARNExpert**: YARN相关文档
- **HistoryCases**: 历史故障案例
- **DockerDocs**: Docker文档（新建）

---

## 七、注意事项

1. **文档编码**: 确保文档是UTF-8编码，特别是中文文档
2. **文档质量**: 优先使用官方文档，确保信息准确性
3. **定期更新**: 文档内容会变化，建议定期更新知识库
4. **存储空间**: 大量文档会占用较多存储空间，注意磁盘容量
5. **导入时间**: 大型文档导入可能需要较长时间，请耐心等待

---

## 八、故障排查

### 问题1: 导入失败，提示"需要安装 langchain_community"

**解决方案**:
```bash
pip install langchain-community
```

### 问题1.1: 文本分割器导入失败

**错误信息**: `No module named 'langchain.text_splitter'` 或 `RecursiveCharacterTextSplitter not found`

**原因**: 在 LangChain 1.0.x 版本中，文本分割器已移至独立的包中。

**解决方案**:
```bash
pip install langchain-text-splitters
```

**说明**: LangChain 1.0.x 将很多功能拆分成了独立的包，`RecursiveCharacterTextSplitter` 现在位于 `langchain_text_splitters` 包中。

### 问题2: PDF文件无法加载

**解决方案**:
```bash
pip install pypdf
# 或
pip install pdfminer.six
```

### 问题3: Markdown文件加载失败

**解决方案**:
```bash
pip install unstructured
# 或使用TextLoader作为备选
```

### 问题4: 中文文档乱码

**解决方案**: 确保文件是UTF-8编码，或在导入时指定正确的编码：
```python
import_document_to_kb(
    file_path="doc.md",
    kb_name="KB",
    encoding="utf-8"  # 或 "gbk", "gb2312" 等
)
```

---

## 九、相关资源链接

### Hadoop资源
- Apache Hadoop官网: https://hadoop.apache.org/
- Hadoop GitHub: https://github.com/apache/hadoop
- Hadoop中文文档: https://hadoop.apache.org/docs/r1.0.4/cn/

### Docker资源
- Docker官网: https://www.docker.com/
- Docker文档: https://docs.docker.com/
- Docker GitHub: https://github.com/docker/docs

### 其他有用的资源
- LangChain文档加载器: https://python.langchain.com/docs/modules/data_connection/document_loaders/
- FAISS文档: https://github.com/facebookresearch/faiss

---

**文档更新时间**: 2025-01-XX
**适用版本**: knowledge_base.py (支持文档导入版本)

