# 故障研究系统代码功能详解

本文档详细讲解阶段一（数据收集）和阶段二（LLM分析）的代码实现和功能。

---

## 阶段一：数据收集模块

### 1. 数据库模块 (`data_collection/database.py`)

#### 1.1 核心功能

数据库模块负责管理SQLite数据库，存储收集的帖子和分析结果。

#### 1.2 关键代码解析

##### 1.2.1 数据库初始化 (`_init_database`)

```python
def _init_database(self):
    """初始化数据库表结构"""
    self.conn = sqlite3.connect(self.db_path)
    self.conn.row_factory = sqlite3.Row  # 使返回结果为字典形式
```

**功能说明**：
- 创建SQLite数据库连接
- 设置`row_factory`为`sqlite3.Row`，使查询结果可以直接转换为字典

**创建的三张表**：

1. **posts表**（存储原始帖子）：
   ```sql
   CREATE TABLE IF NOT EXISTS posts (
       id INTEGER PRIMARY KEY AUTOINCREMENT,
       source TEXT NOT NULL,           -- 来源平台
       post_id TEXT NOT NULL,          -- 原始平台ID
       title TEXT NOT NULL,            -- 标题
       content TEXT,                   -- 内容（HTML格式）
       tags TEXT,                      -- 标签（JSON格式）
       url TEXT NOT NULL,              -- URL
       author TEXT,                    -- 作者
       created_at TIMESTAMP,          -- 帖子创建时间
       collected_at TIMESTAMP,        -- 收集时间
       view_count INTEGER,            -- 浏览量
       score INTEGER,                 -- 分数
       answer_count INTEGER,          -- 回答数
       is_accepted INTEGER,           -- 是否有被接受的答案
       UNIQUE(source, post_id)        -- 防止重复插入
   )
   ```

2. **fault_analysis表**（存储LLM分析结果）：
   ```sql
   CREATE TABLE IF NOT EXISTS fault_analysis (
       post_id INTEGER,                -- 关联posts表
       is_real_fault INTEGER,          -- 是否为真实故障
       fault_component TEXT,           -- 故障组件
       fault_symptoms TEXT,            -- 故障症状
       error_logs TEXT,                -- 错误日志
       root_cause TEXT,                -- 根本原因
       solution TEXT,                  -- 解决方案
       preliminary_category TEXT,      -- 初步分类
       ...
   )
   ```

3. **fault_type_mapping表**（存储最终分类结果，待使用）

##### 1.2.2 插入帖子 (`insert_post`)

```python
def insert_post(self, post_data: Dict[str, Any]) -> int:
    # 处理tags（如果是列表，转换为JSON字符串）
    tags = post_data.get("tags", [])
    if isinstance(tags, list):
        tags = json.dumps(tags)
    
    # 使用 INSERT OR IGNORE 防止重复
    cursor.execute("""
        INSERT OR IGNORE INTO posts (...)
        VALUES (?, ?, ?, ...)
    """, (...))
```

**功能说明**：
- 接收帖子数据字典，自动处理数据类型转换
- 使用`INSERT OR IGNORE`防止重复插入（基于`source`和`post_id`的唯一约束）
- 如果帖子已存在，返回现有记录的ID；否则返回新插入的ID

**关键特性**：
- **去重机制**：通过`UNIQUE(source, post_id)`约束自动去重
- **数据类型处理**：自动将列表转换为JSON字符串，datetime转换为ISO格式
- **错误处理**：捕获异常并回滚事务

##### 1.2.3 查询帖子 (`get_all_posts`)

```python
def get_all_posts(self, source: Optional[str] = None, limit: Optional[int] = None):
    query = "SELECT * FROM posts"
    if source:
        query += " WHERE source = ?"
    query += " ORDER BY collected_at DESC"
    if limit:
        query += " LIMIT ?"
```

**功能说明**：
- 支持按来源筛选（`source`参数）
- 支持限制返回数量（`limit`参数）
- 按收集时间倒序排列（最新的在前）
- 自动解析tags字段的JSON字符串为列表

---

### 2. Stack Overflow收集器 (`data_collection/stackoverflow_collector.py`)

#### 2.1 核心功能

使用Stack Exchange API收集Hadoop相关的故障帖子。

#### 2.2 关键代码解析

##### 2.2.1 初始化配置

```python
BASE_URL = "https://api.stackexchange.com/2.3"

HADOOP_TAGS = [
    "hadoop",
    "hdfs",
    "yarn",
    "mapreduce",
    "apache-hadoop"
]

SEARCH_KEYWORDS = [
    "hadoop error",
    "hadoop exception",
    "hadoop failure",
    "hdfs error",
    ...
]
```

**功能说明**：
- 定义了Stack Exchange API的基础URL
- 预定义了Hadoop相关的标签和搜索关键词
- 这些标签和关键词用于多维度搜索，确保收集到足够的故障相关帖子

##### 2.2.2 搜索问题 (`search_questions`)

```python
def search_questions(self, query: Optional[str] = None,
                    tagged: Optional[List[str]] = None,
                    pages: int = 1, page_size: int = 50):
    for page in range(1, pages + 1):
        params = {
            "order": "desc",
            "sort": "relevance",      # 按相关性排序
            "site": "stackoverflow",
            "pagesize": min(page_size, 100),  # API限制最大100
            "page": page,
            "filter": "withbody"      # 包含问题正文
        }
        
        if query:
            params["q"] = query       # 关键词搜索
        if tagged:
            params["tagged"] = ";".join(tagged)  # 标签搜索
        
        response = self.session.get(
            f"{self.BASE_URL}/search/advanced",
            params=params,
            timeout=30
        )
```

**功能说明**：
- 支持两种搜索方式：
  1. **标签搜索**：通过`tagged`参数搜索特定标签的问题
  2. **关键词搜索**：通过`q`参数进行全文搜索
- **分页处理**：支持多页获取，每页最多100个问题（API限制）
- **Rate Limit处理**：
  ```python
  if "quota_remaining" in data:
      remaining = data["quota_remaining"]
      if remaining < 10:
          print("⚠️ API 配额即将用完，暂停收集")
          break
  time.sleep(self.delay)  # 延迟避免触发rate limit
  ```

##### 2.2.3 解析问题数据 (`parse_question`)

```python
def parse_question(self, question: Dict[str, Any]) -> Dict[str, Any]:
    # 提取标签
    tags = question.get("tags", [])
    
    # 提取内容（HTML格式）
    content = question.get("body", "")
    
    # 提取时间（Unix时间戳转换为datetime）
    created_timestamp = question.get("creation_date", 0)
    created_at = datetime.fromtimestamp(created_timestamp) if created_timestamp else None
    
    # 检查是否有被接受的答案
    is_accepted = 1 if question.get("accepted_answer_id") else 0
```

**功能说明**：
- 将Stack Overflow API返回的原始数据转换为标准格式
- 处理时间戳转换、标签提取等数据清洗工作
- 判断帖子质量（是否有被接受的答案）

##### 2.2.4 批量收集 (`collect_posts`)

```python
def collect_posts(self, target_count: int = 500) -> int:
    # 策略1: 使用标签搜索
    for tag in self.HADOOP_TAGS:
        questions = self.search_questions(tagged=[tag], pages=5, page_size=30)
        # 去重（基于question_id）
        existing_ids = {q.get("question_id") for q in all_questions}
        new_questions = [q for q in questions if q.get("question_id") not in existing_ids]
        all_questions.extend(new_questions)
    
    # 策略2: 使用关键词搜索
    for keyword in self.SEARCH_KEYWORDS:
        questions = self.search_questions(query=keyword, pages=3, page_size=30)
        # 去重...
    
    # 按分数排序，优先收集高分问题
    all_questions.sort(key=lambda x: x.get("score", 0), reverse=True)
    
    # 保存到数据库
    for question in all_questions[:target_count]:
        post_data = self.parse_question(question)
        self.db.insert_post(post_data)
```

**功能说明**：
- **双重搜索策略**：
  1. 标签搜索：覆盖所有Hadoop相关标签
  2. 关键词搜索：使用故障相关关键词
- **去重机制**：基于`question_id`自动去重
- **质量排序**：按分数排序，优先收集高质量帖子
- **批量保存**：将解析后的数据批量保存到数据库

**收集流程**：
```
1. 标签搜索 → 获取大量相关帖子
2. 关键词搜索 → 补充故障相关帖子
3. 去重 → 移除重复帖子
4. 排序 → 按分数排序
5. 保存 → 保存到数据库（自动去重）
```

---

## 阶段二：LLM分析模块

### 3. LLM分析器 (`data_analysis/llm_analyzer.py`)

#### 3.1 核心功能

使用LLM批量分析收集的帖子，筛选真实故障场景并提取关键信息。

#### 3.2 关键代码解析

##### 3.2.1 初始化

```python
def __init__(self, db: FaultResearchDB, llm_client: LLMClient, batch_size: int = 10):
    self.db = db
    self.llm_client = llm_client
    self.batch_size = batch_size
```

**功能说明**：
- 接收数据库实例和LLM客户端
- `batch_size`用于控制批处理大小（当前未使用，可扩展）

##### 3.2.2 HTML内容清理 (`_clean_html_content`)

```python
def _clean_html_content(self, content: str) -> str:
    # 移除HTML实体（如 &quot; → "）
    content = html.unescape(content)
    
    # 移除HTML标签
    content = re.sub(r'<[^>]+>', '', content)
    
    # 清理多余空白
    content = re.sub(r'\s+', ' ', content).strip()
```

**功能说明**：
- Stack Overflow的帖子内容是HTML格式，需要清理为纯文本
- 移除HTML标签和实体，保留纯文本内容
- 清理多余空白，使文本更易读

##### 3.2.3 构建分析提示词 (`_build_analysis_prompt`)

```python
def _build_analysis_prompt(self, post: Dict[str, Any]) -> str:
    title = post.get('title', '')
    content = self._clean_html_content(post.get('content', ''))
    
    # 限制内容长度（避免token过多）
    if len(content) > 2000:
        content = content[:2000] + "..."
    
    prompt = f"""请分析以下Stack Overflow帖子，判断它是否描述了一个真实的Hadoop集群故障场景。

帖子标题: {title}
标签: {tags}
URL: {url}

帖子内容:
{content}

请按照以下JSON格式回答：
{{
    "is_real_fault": true/false,
    "fault_component": "HDFS/YARN/MapReduce/Network/Generic/Unknown",
    "fault_symptoms": "简要描述故障症状",
    "error_logs": "提取的错误日志或异常信息",
    "root_cause": "推测的根本原因",
    "solution": "解决方案或建议",
    "preliminary_category": "初步分类",
    "confidence": 0.0-1.0,
    ...
}}
"""
```

**功能说明**：
- 构建结构化的分析提示词，指导LLM进行标准化分析
- 限制内容长度，避免token过多导致API调用失败
- 要求LLM返回JSON格式，便于后续解析

**分析维度**：
1. **故障判断**：是否为真实故障场景
2. **组件识别**：HDFS/YARN/MapReduce等
3. **症状提取**：故障表现
4. **日志提取**：错误日志和异常信息
5. **原因分析**：根本原因推测
6. **解决方案**：解决建议
7. **初步分类**：故障类型分类

##### 3.2.4 分析单个帖子 (`analyze_post`)

```python
def analyze_post(self, post: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    prompt = self._build_analysis_prompt(post)
    
    # 调用LLM
    response = self.llm_client.generate(
        prompt=prompt,
        system_prompt="你是一个专业的Hadoop集群故障分析专家...",
        temperature=0.1  # 低温度以保证一致性
    )
    
    # 解析JSON响应
    analysis_result = self._parse_llm_response(response)
    
    if analysis_result:
        # 保存到数据库
        self._save_analysis(post['id'], analysis_result)
        return analysis_result
```

**功能说明**：
- 调用LLM进行分析
- 使用低温度（0.1）保证分析结果的一致性
- 解析LLM返回的JSON响应
- 将分析结果保存到数据库

##### 3.2.5 解析LLM响应 (`_parse_llm_response`)

```python
def _parse_llm_response(self, response: str) -> Optional[Dict[str, Any]]:
    import json
    try:
        # 尝试1: 直接解析JSON
        if response.strip().startswith('{'):
            return json.loads(response)
        
        # 尝试2: 从markdown代码块中提取
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        
        # 尝试3: 提取第一个JSON对象
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
    except Exception as e:
        print(f"解析响应失败: {e}")
        return None
```

**功能说明**：
- **多重解析策略**：处理LLM可能返回的不同格式
  1. 直接JSON格式
  2. Markdown代码块中的JSON
  3. 文本中的JSON对象
- **容错处理**：如果解析失败，返回None并记录错误

##### 3.2.6 批量分析 (`analyze_all_posts`)

```python
def analyze_all_posts(self, source: Optional[str] = None,
                     limit: Optional[int] = None,
                     delay: float = 1.0):
    # 获取待分析的帖子
    posts = self.db.get_all_posts(source=source, limit=limit)
    
    # 检查哪些已经分析过
    cursor.execute("SELECT post_id FROM fault_analysis")
    analyzed_post_ids = {row[0] for row in cursor.fetchall()}
    
    # 过滤出未分析的帖子
    unanalyzed_posts = [p for p in posts if p['id'] not in analyzed_post_ids]
    
    # 批量分析
    for i, post in enumerate(unanalyzed_posts, 1):
        result = self.analyze_post(post)
        time.sleep(delay)  # 延迟避免API限制
```

**功能说明**：
- **智能跳过**：自动跳过已分析的帖子，支持断点续传
- **批量处理**：支持批量分析所有帖子
- **进度显示**：显示分析进度和结果
- **延迟控制**：每次分析后延迟，避免触发API rate limit

**分析流程**：
```
1. 获取所有帖子
2. 检查已分析列表 → 过滤未分析帖子
3. 逐个分析 → 调用LLM
4. 解析结果 → 提取JSON
5. 保存到数据库 → fault_analysis表
6. 延迟 → 避免API限制
```

##### 3.2.7 统计分析 (`get_analysis_statistics`)

```python
def get_analysis_statistics(self) -> Dict[str, Any]:
    # 总分析数
    cursor.execute("SELECT COUNT(*) FROM fault_analysis")
    total_analyzed = cursor.fetchone()[0]
    
    # 真实故障数
    cursor.execute("SELECT COUNT(*) FROM fault_analysis WHERE is_real_fault = 1")
    real_faults = cursor.fetchone()[0]
    
    # 按组件统计
    cursor.execute("""
        SELECT fault_component, COUNT(*) as count
        FROM fault_analysis
        WHERE is_real_fault = 1
        GROUP BY fault_component
        ORDER BY count DESC
    """)
    component_stats = {...}
    
    # 按分类统计
    cursor.execute("""
        SELECT preliminary_category, COUNT(*) as count
        FROM fault_analysis
        WHERE is_real_fault = 1
        GROUP BY preliminary_category
        ORDER BY count DESC
    """)
```

**功能说明**：
- 提供分析统计信息
- 包括：总分析数、真实故障数、组件分布、分类分布等
- 用于评估数据质量和分析效果

---

## 数据流转过程

### 完整流程

```
1. 数据收集（阶段一）
   Stack Overflow API
        ↓
   StackOverflowCollector.search_questions()
        ↓
   StackOverflowCollector.parse_question()
        ↓
   FaultResearchDB.insert_post()
        ↓
   SQLite数据库 (posts表)

2. LLM分析（阶段二）
   SQLite数据库 (posts表)
        ↓
   LLMAnalyzer.analyze_all_posts()
        ↓
   LLMAnalyzer.analyze_post()
        ↓
   LLMClient.generate() → LLM API
        ↓
   LLMAnalyzer._parse_llm_response()
        ↓
   LLMAnalyzer._save_analysis()
        ↓
   SQLite数据库 (fault_analysis表)
```

### 关键设计特点

1. **去重机制**：
   - 数据库层面：`UNIQUE(source, post_id)`约束
   - 收集层面：基于`question_id`的去重
   - 分析层面：检查`fault_analysis`表，跳过已分析帖子

2. **容错处理**：
   - API调用异常处理
   - JSON解析失败处理
   - 数据库操作回滚

3. **性能优化**：
   - 数据库索引（source, collected_at, post_id）
   - 批量处理支持
   - Rate limit控制

4. **可扩展性**：
   - 模块化设计，易于扩展
   - 支持多数据源（Stack Overflow、CSDN）
   - 支持多LLM模型（qwen-8b、gpt-4o、deepseek-r1）

---

## 使用示例

### 阶段一：收集数据

```python
from data_collection.database import FaultResearchDB
from data_collection.stackoverflow_collector import StackOverflowCollector

# 初始化数据库
db = FaultResearchDB("fault_research.db")

# 创建收集器
collector = StackOverflowCollector(db, delay=0.2)

# 收集500个帖子
collected = collector.collect_posts(target_count=500)
print(f"成功收集 {collected} 个帖子")
```

### 阶段二：LLM分析

```python
from data_collection.database import FaultResearchDB
from data_analysis.llm_analyzer import LLMAnalyzer
from mutli_agent.llm_client import LLMClient

# 初始化数据库
db = FaultResearchDB("fault_research.db")

# 初始化LLM客户端
llm_client = LLMClient(model_name="qwen-8b")

# 创建分析器
analyzer = LLMAnalyzer(db, llm_client)

# 批量分析（先测试10个）
stats = analyzer.analyze_all_posts(limit=10, delay=1.0)

# 查看统计
analysis_stats = analyzer.get_analysis_statistics()
print(f"真实故障: {analysis_stats['real_faults']}")
print(f"组件分布: {analysis_stats['component_stats']}")
```

---

## 总结

### 阶段一（数据收集）的核心价值

1. **自动化收集**：无需手动查找和复制帖子
2. **多维度搜索**：标签+关键词双重策略，确保覆盖面
3. **质量保证**：按分数排序，优先收集高质量帖子
4. **数据持久化**：SQLite数据库，便于后续分析

### 阶段二（LLM分析）的核心价值

1. **智能筛选**：自动识别真实故障场景，过滤噪音
2. **信息提取**：结构化提取故障症状、错误日志、根本原因等
3. **初步分类**：按组件和故障类型进行初步分类
4. **批量处理**：支持大规模批量分析，自动跳过已分析项

### 整体架构优势

- **模块化**：各模块职责清晰，易于维护和扩展
- **容错性**：完善的异常处理和错误恢复机制
- **可扩展**：支持多数据源、多LLM模型
- **数据驱动**：基于真实案例，而非理论假设
