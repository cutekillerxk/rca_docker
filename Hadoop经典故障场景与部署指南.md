# Hadoopç»å…¸æ•…éšœåœºæ™¯ä¸éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [Hadoopç»„ä»¶è¯¦è§£ï¼ˆå¿…è¯»ï¼‰](#hadoopç»„ä»¶è¯¦è§£å¿…è¯»)
2. [æ•…éšœåœºæ™¯æ€»è§ˆ](#æ•…éšœåœºæ™¯æ€»è§ˆ)
3. [éƒ¨ç½²å‡†å¤‡](#éƒ¨ç½²å‡†å¤‡)
4. [è¯¦ç»†åœºæ™¯ä¸éƒ¨ç½²æŒ‡å¯¼](#è¯¦ç»†åœºæ™¯ä¸éƒ¨ç½²æŒ‡å¯¼)

---

## Hadoopç»„ä»¶è¯¦è§£ï¼ˆå¿…è¯»ï¼‰

> ğŸ’¡ **æœ¬èŠ‚ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡ŠHadoopå„ç»„ä»¶çš„åŠŸèƒ½ï¼Œå³ä½¿ä½ å®Œå…¨ä¸äº†è§£Hadoopä¹Ÿèƒ½çœ‹æ‡‚ã€‚**

### ğŸ¯ Hadoopæ˜¯ä»€ä¹ˆï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æœ‰ä¸€ä¸ª**è¶…å¤§çš„æ–‡ä»¶**ï¼ˆæ¯”å¦‚1TBï¼‰ï¼Œä½ çš„ç”µè„‘å­˜ä¸ä¸‹ï¼Œæ€ä¹ˆåŠï¼Ÿ

**ä¼ ç»Ÿæ–¹æ³•**ï¼šä¹°æ›´å¤§çš„ç¡¬ç›˜ âŒï¼ˆè´µä¸”æœ‰é™ï¼‰

**Hadoopæ–¹æ³•**ï¼šæŠŠæ–‡ä»¶**åˆ‡åˆ†æˆå¾ˆå¤šå°å—**ï¼Œåˆ†åˆ«å­˜åˆ°**å¤šå°ç”µè„‘**ä¸Šï¼Œéœ€è¦æ—¶å†**æ‹¼å›æ¥** âœ…

è¿™å°±æ˜¯**åˆ†å¸ƒå¼å­˜å‚¨**çš„æ ¸å¿ƒæ€æƒ³ã€‚

---

### ğŸ“¦ ç»„ä»¶1ï¼šHDFSï¼ˆHadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼‰

#### ç”¨ç”Ÿæ´»ä¾‹å­ç†è§£

**HDFSå°±åƒä¸€ä¸ªå¤§å‹å›¾ä¹¦é¦†ç³»ç»Ÿ**ï¼š

- **NameNodeï¼ˆåç§°èŠ‚ç‚¹ï¼‰** = **å›¾ä¹¦ç®¡ç†å‘˜**
  - è®°ä½æ¯æœ¬ä¹¦ï¼ˆæ–‡ä»¶ï¼‰æ”¾åœ¨å“ªä¸ªä¹¦æ¶ï¼ˆDataNodeï¼‰ä¸Š
  - è®°ä½æ¯æœ¬ä¹¦çš„åå­—ã€å¤§å°ã€ä½ç½®
  - **ä¸å­˜å®é™…çš„ä¹¦**ï¼Œåªå­˜"ç›®å½•ç´¢å¼•"
  
- **DataNodeï¼ˆæ•°æ®èŠ‚ç‚¹ï¼‰** = **å®é™…çš„ä¹¦æ¶**
  - çœŸæ­£å­˜æ”¾ä¹¦ï¼ˆæ•°æ®ï¼‰çš„åœ°æ–¹
  - æ¯ä¸ªä¹¦æ¶ï¼ˆDataNodeï¼‰å­˜ä¸€éƒ¨åˆ†ä¹¦
  - å®šæœŸå‘ç®¡ç†å‘˜ï¼ˆNameNodeï¼‰æ±‡æŠ¥ï¼š"æˆ‘è¿™é‡Œæœ‰è¿™äº›ä¹¦"
  
- **SecondaryNameNodeï¼ˆè¾…åŠ©åç§°èŠ‚ç‚¹ï¼‰** = **å‰¯ç®¡ç†å‘˜**
  - å¸®åŠ©NameNodeæ•´ç†ç›®å½•ï¼Œå‡è½»è´Ÿæ‹…
  - å®šæœŸå¤‡ä»½NameNodeçš„"ç›®å½•ç´¢å¼•"

#### æŠ€æœ¯ç»†èŠ‚

| ç»„ä»¶ | ä½œç”¨ | ç±»æ¯” |
|------|------|------|
| **NameNode** | å­˜å‚¨æ–‡ä»¶ç³»ç»Ÿçš„å…ƒæ•°æ®ï¼ˆæ–‡ä»¶åã€ç›®å½•ç»“æ„ã€æ–‡ä»¶å—ä½ç½®ï¼‰ | å›¾ä¹¦é¦†çš„ç›®å½•å¡ç‰‡ç³»ç»Ÿ |
| **DataNode** | å­˜å‚¨å®é™…çš„æ•°æ®å— | å›¾ä¹¦é¦†çš„ä¹¦æ¶ |
| **SecondaryNameNode** | å®šæœŸåˆå¹¶NameNodeçš„ç¼–è¾‘æ—¥å¿—ï¼Œç”Ÿæˆæ–°çš„é•œåƒæ–‡ä»¶ | æ•´ç†ç›®å½•çš„åŠ©æ‰‹ |

#### ä¸ºä»€ä¹ˆéœ€è¦HDFSï¼Ÿ

1. **å¤§æ–‡ä»¶å­˜å‚¨**ï¼šå•ä¸ªæ–‡ä»¶å¯ä»¥è¶…è¿‡å•æœºç¡¬ç›˜å®¹é‡
2. **å®¹é”™æ€§**ï¼šæ¯ä¸ªæ•°æ®å—æœ‰å¤šä¸ªå‰¯æœ¬ï¼ˆé»˜è®¤3ä¸ªï¼‰ï¼Œä¸€ä¸ªDataNodeåäº†ï¼Œæ•°æ®è¿˜åœ¨
3. **é«˜ååé‡**ï¼šå¤šå°æœºå™¨å¹¶è¡Œè¯»å†™ï¼Œé€Ÿåº¦å¿«

---

### âš™ï¸ ç»„ä»¶2ï¼šYARNï¼ˆèµ„æºç®¡ç†å™¨ï¼‰

#### ç”¨ç”Ÿæ´»ä¾‹å­ç†è§£

**YARNå°±åƒä¸€ä¸ªå·¥å‚çš„è°ƒåº¦ç³»ç»Ÿ**ï¼š

- **ResourceManagerï¼ˆèµ„æºç®¡ç†å™¨ï¼‰** = **å·¥å‚å‚é•¿**
  - çŸ¥é“å·¥å‚æœ‰å¤šå°‘å·¥äººï¼ˆCPUï¼‰ã€å¤šå°‘ææ–™ï¼ˆå†…å­˜ï¼‰
  - å†³å®šå“ªä¸ªä»»åŠ¡åˆ†é…ç»™å“ªä¸ªè½¦é—´ï¼ˆNodeManagerï¼‰
  - åªæœ‰ä¸€ä¸ªï¼Œç®¡ç†æ•´ä¸ªå·¥å‚
  
- **NodeManagerï¼ˆèŠ‚ç‚¹ç®¡ç†å™¨ï¼‰** = **è½¦é—´ä¸»ä»»**
  - ç®¡ç†è‡ªå·±è½¦é—´çš„å·¥äººå’Œææ–™
  - å‘å‚é•¿æ±‡æŠ¥ï¼š"æˆ‘è¿™é‡Œæœ‰2ä¸ªå·¥äººï¼Œ4GBææ–™å¯ç”¨"
  - æ¯ä¸ªèŠ‚ç‚¹ï¼ˆæœºå™¨ï¼‰éƒ½æœ‰ä¸€ä¸ª
  
- **Containerï¼ˆå®¹å™¨ï¼‰** = **å·¥ä½œå°**
  - åˆ†é…ç»™å…·ä½“ä»»åŠ¡çš„èµ„æºï¼ˆCPU + å†…å­˜ï¼‰
  - ä»»åŠ¡åœ¨è¿™ä¸ª"å·¥ä½œå°"ä¸Šè¿è¡Œ
  - ä»»åŠ¡å®Œæˆåï¼Œ"å·¥ä½œå°"å›æ”¶ç»™å…¶ä»–ä»»åŠ¡ç”¨

#### æŠ€æœ¯ç»†èŠ‚

| ç»„ä»¶ | ä½œç”¨ | ç±»æ¯” |
|------|------|------|
| **ResourceManager** | ç®¡ç†æ•´ä¸ªé›†ç¾¤çš„èµ„æºï¼ˆCPUã€å†…å­˜ï¼‰ï¼Œå†³å®šä»»åŠ¡åˆ†é…ç»™å“ªä¸ªèŠ‚ç‚¹ | å·¥å‚å‚é•¿ |
| **NodeManager** | ç®¡ç†å•ä¸ªèŠ‚ç‚¹çš„èµ„æºï¼Œå‘ResourceManageræ±‡æŠ¥ï¼Œæ‰§è¡Œä»»åŠ¡ | è½¦é—´ä¸»ä»» |
| **Container** | åˆ†é…ç»™ä»»åŠ¡çš„èµ„æºå•ä½ï¼ˆå¦‚ï¼š2æ ¸CPU + 4GBå†…å­˜ï¼‰ | å·¥ä½œå° |

#### ä¸ºä»€ä¹ˆéœ€è¦YARNï¼Ÿ

1. **èµ„æºç®¡ç†**ï¼šå¤šä¸ªäººåŒæ—¶æäº¤ä»»åŠ¡ï¼ŒYARNå…¬å¹³åˆ†é…èµ„æº
2. **ä»»åŠ¡è°ƒåº¦**ï¼šå†³å®šå“ªä¸ªä»»åŠ¡å…ˆè¿è¡Œï¼Œå“ªä¸ªä»»åŠ¡ç­‰å¾…
3. **èµ„æºéš”ç¦»**ï¼šæ¯ä¸ªä»»åŠ¡æœ‰ç‹¬ç«‹çš„èµ„æºï¼Œä¸ä¼šäº’ç›¸å¹²æ‰°

---

### ğŸ”„ ç»„ä»¶3ï¼šMapReduceï¼ˆè®¡ç®—æ¡†æ¶ï¼‰

#### ç”¨ç”Ÿæ´»ä¾‹å­ç†è§£

**MapReduceå°±åƒç»Ÿè®¡å…¨æ ¡å­¦ç”Ÿæˆç»©çš„è¿‡ç¨‹**ï¼š

**åœºæ™¯**ï¼šç»Ÿè®¡æ¯ä¸ªç­çº§çš„å¹³å‡åˆ†

**ä¼ ç»Ÿæ–¹æ³•**ï¼ˆä¸€ä¸ªäººåšï¼‰ï¼š
1. æ‹¿åˆ°æ‰€æœ‰å­¦ç”Ÿçš„æˆç»©å•
2. ä¸€ä¸ªä¸€ä¸ªçœ‹ï¼Œç´¯åŠ æ¯ä¸ªç­çš„åˆ†æ•°
3. è®¡ç®—å¹³å‡å€¼
4. **å¤ªæ…¢äº†ï¼** âŒ

**MapReduceæ–¹æ³•**ï¼ˆå¤šäººå¹¶è¡Œï¼‰ï¼š
1. **Mapé˜¶æ®µï¼ˆæ˜ å°„ï¼‰**ï¼š
   - æŠŠä»»åŠ¡åˆ†ç»™å¤šä¸ªè€å¸ˆ
   - è€å¸ˆAç»Ÿè®¡1-3ç­ï¼Œè€å¸ˆBç»Ÿè®¡4-6ç­ï¼Œè€å¸ˆCç»Ÿè®¡7-9ç­
   - **å¹¶è¡Œå¤„ç†**ï¼Œé€Ÿåº¦å¿« âœ…
   
2. **Shuffleé˜¶æ®µï¼ˆæ´—ç‰Œï¼‰**ï¼š
   - æŠŠç›¸åŒç­çº§çš„æˆç»©æ”¶é›†åˆ°ä¸€èµ·
   - æ¯”å¦‚ï¼šæ‰€æœ‰"1ç­"çš„æˆç»©æ”¾åœ¨ä¸€èµ·
   
3. **Reduceé˜¶æ®µï¼ˆå½’çº¦ï¼‰**ï¼š
   - æ¯ä¸ªè€å¸ˆè®¡ç®—è‡ªå·±è´Ÿè´£ç­çº§çš„å¹³å‡åˆ†
   - è€å¸ˆAè®¡ç®—1ç­å¹³å‡åˆ†ï¼Œè€å¸ˆBè®¡ç®—2ç­å¹³å‡åˆ†...
   - **å¹¶è¡Œè®¡ç®—**ï¼Œé€Ÿåº¦å¿« âœ…

#### æŠ€æœ¯ç»†èŠ‚

| é˜¶æ®µ | ä½œç”¨ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| **Map** | å°†è¾“å…¥æ•°æ®åˆ‡åˆ†æˆå°å—ï¼Œå¹¶è¡Œå¤„ç†ï¼Œç”Ÿæˆé”®å€¼å¯¹ | åŸå§‹æ•°æ® | (key, value) å¯¹ |
| **Shuffle** | å°†ç›¸åŒkeyçš„æ•°æ®æ”¶é›†åˆ°ä¸€èµ·ï¼Œå‘é€ç»™åŒä¸€ä¸ªReducer | Mapè¾“å‡º | æŒ‰keyåˆ†ç»„çš„æ•°æ® |
| **Reduce** | å¯¹æ¯ä¸ªkeyçš„æ‰€æœ‰valueè¿›è¡Œèšåˆè®¡ç®—ï¼ˆæ±‚å’Œã€å¹³å‡ç­‰ï¼‰ | åˆ†ç»„åçš„æ•°æ® | æœ€ç»ˆç»“æœ |

#### ç»å…¸ä¾‹å­ï¼šWordCountï¼ˆè¯é¢‘ç»Ÿè®¡ï¼‰

**ä»»åŠ¡**ï¼šç»Ÿè®¡ä¸€ç¯‡æ–‡ç« ä¸­æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°

**è¾“å…¥**ï¼š
```
hello world
hello hadoop
world mapreduce
```

**Mapé˜¶æ®µ**ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰ï¼š
```
Mapä»»åŠ¡1: "hello world" â†’ (hello, 1), (world, 1)
Mapä»»åŠ¡2: "hello hadoop" â†’ (hello, 1), (hadoop, 1)
Mapä»»åŠ¡3: "world mapreduce" â†’ (world, 1), (mapreduce, 1)
```

**Shuffleé˜¶æ®µ**ï¼ˆæŒ‰keyåˆ†ç»„ï¼‰ï¼š
```
hello â†’ [1, 1]
world â†’ [1, 1]
hadoop â†’ [1]
mapreduce â†’ [1]
```

**Reduceé˜¶æ®µ**ï¼ˆæ±‚å’Œï¼‰ï¼š
```
hello â†’ 2
world â†’ 2
hadoop â†’ 1
mapreduce â†’ 1
```

#### ä¸ºä»€ä¹ˆéœ€è¦MapReduceï¼Ÿ

1. **å¹¶è¡Œè®¡ç®—**ï¼šå¤§æ•°æ®åˆ†æˆå°å—ï¼Œå¤šå°æœºå™¨åŒæ—¶å¤„ç†
2. **å®¹é”™æ€§**ï¼šæŸä¸ªä»»åŠ¡å¤±è´¥ï¼Œè‡ªåŠ¨é‡æ–°æ‰§è¡Œ
3. **ç®€å•ç¼–ç¨‹æ¨¡å‹**ï¼šåªéœ€å†™Mapå’ŒReduceå‡½æ•°ï¼Œæ¡†æ¶å¤„ç†åˆ†å¸ƒå¼ç»†èŠ‚

---

### ğŸ”— ç»„ä»¶ä¹‹é—´çš„å…³ç³»

#### å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹

```
ç”¨æˆ·æäº¤ä»»åŠ¡
    â†“
YARN ResourceManager æ¥æ”¶ä»»åŠ¡
    â†“
YARN ResourceManager åˆ†é…èµ„æºï¼ˆContainerï¼‰
    â†“
YARN NodeManager å¯åŠ¨ Container
    â†“
MapReduce ApplicationMaster å¯åŠ¨
    â†“
MapReduce ä» HDFS è¯»å–æ•°æ®
    â†“
MapReduce æ‰§è¡Œ Map é˜¶æ®µ
    â†“
MapReduce æ‰§è¡Œ Shuffle é˜¶æ®µ
    â†“
MapReduce æ‰§è¡Œ Reduce é˜¶æ®µ
    â†“
MapReduce å°†ç»“æœå†™å› HDFS
    â†“
ä»»åŠ¡å®Œæˆ
```

#### ç”¨ç”Ÿæ´»ä¾‹å­ç†è§£æ•´ä¸ªæµç¨‹

**åœºæ™¯**ï¼šåˆ†æ100GBçš„æ—¥å¿—æ–‡ä»¶ï¼Œç»Ÿè®¡æ¯ä¸ªIPçš„è®¿é—®æ¬¡æ•°

1. **ç”¨æˆ·æäº¤ä»»åŠ¡** â†’ "æˆ‘è¦åˆ†ææ—¥å¿—"
2. **YARN ResourceManager** â†’ "å¥½çš„ï¼Œæˆ‘ç»™ä½ åˆ†é…èµ„æº"
3. **YARN NodeManager** â†’ "æˆ‘è¿™é‡Œæœ‰èµ„æºï¼Œå¯ä»¥è¿è¡Œä»»åŠ¡"
4. **MapReduce ApplicationMaster** â†’ "æˆ‘æ¥åè°ƒæ•´ä¸ªä»»åŠ¡"
5. **ä»HDFSè¯»å–æ•°æ®** â†’ "ä»åˆ†å¸ƒå¼å­˜å‚¨è¯»å–100GBæ—¥å¿—"
6. **Mapé˜¶æ®µ** â†’ "100ä¸ªMapä»»åŠ¡å¹¶è¡Œå¤„ç†ï¼Œæ¯ä¸ªå¤„ç†1GB"
7. **Shuffleé˜¶æ®µ** â†’ "æŠŠç›¸åŒIPçš„è®¿é—®è®°å½•æ”¶é›†åˆ°ä¸€èµ·"
8. **Reduceé˜¶æ®µ** â†’ "ç»Ÿè®¡æ¯ä¸ªIPçš„è®¿é—®æ¬¡æ•°"
9. **å†™å›HDFS** â†’ "æŠŠç»“æœä¿å­˜åˆ°åˆ†å¸ƒå¼å­˜å‚¨"
10. **ä»»åŠ¡å®Œæˆ** â†’ "åˆ†æå®Œæˆï¼"

---

### ğŸ“Š ç»„ä»¶å¯¹æ¯”æ€»ç»“

| ç»„ä»¶ | ä¸»è¦åŠŸèƒ½ | ç±»æ¯” | æ˜¯å¦å¿…é¡» |
|------|---------|------|----------|
| **HDFS** | åˆ†å¸ƒå¼å­˜å‚¨ | å›¾ä¹¦é¦†ç³»ç»Ÿ | âœ… å¿…é¡»ï¼ˆå­˜å‚¨æ•°æ®ï¼‰ |
| **YARN** | èµ„æºç®¡ç† | å·¥å‚è°ƒåº¦ç³»ç»Ÿ | âœ… å¿…é¡»ï¼ˆè¿è¡Œä»»åŠ¡ï¼‰ |
| **MapReduce** | è®¡ç®—æ¡†æ¶ | ç»Ÿè®¡æµç¨‹ | âš ï¸ å¯é€‰ï¼ˆå¯ä»¥ç”¨Sparkç­‰æ›¿ä»£ï¼‰ |

---

### ğŸ“ å­¦ä¹ è·¯å¾„å»ºè®®

1. **å…ˆç†è§£HDFS**ï¼šæ•°æ®æ€ä¹ˆå­˜ã€æ€ä¹ˆå–
2. **å†ç†è§£YARN**ï¼šä»»åŠ¡æ€ä¹ˆåˆ†é…èµ„æºã€æ€ä¹ˆè¿è¡Œ
3. **æœ€åç†è§£MapReduce**ï¼šä»»åŠ¡å…·ä½“æ€ä¹ˆè®¡ç®—
4. **å®è·µ**ï¼šè¿è¡Œä¸€ä¸ªç®€å•çš„WordCountä»»åŠ¡ï¼Œè§‚å¯Ÿæ•´ä¸ªè¿‡ç¨‹

---

### ğŸ” å¸¸è§é—®é¢˜è§£ç­”

#### Q1: HDFSå’Œæ™®é€šæ–‡ä»¶ç³»ç»Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**æ™®é€šæ–‡ä»¶ç³»ç»Ÿ**ï¼ˆå¦‚Windowsçš„Cç›˜ï¼‰ï¼š
- æ–‡ä»¶å­˜åœ¨ä¸€å°ç”µè„‘ä¸Š
- è¿™å°ç”µè„‘åäº†ï¼Œæ–‡ä»¶å°±ä¸¢äº†
- æ–‡ä»¶å¤ªå¤§ï¼ˆæ¯”å¦‚1TBï¼‰ï¼Œå•æœºå­˜ä¸ä¸‹

**HDFS**ï¼š
- æ–‡ä»¶åˆ‡åˆ†æˆå¾ˆå¤šå—ï¼Œå­˜åœ¨å¤šå°ç”µè„‘ä¸Š
- æ¯å—æœ‰å¤šä¸ªå‰¯æœ¬ï¼Œä¸€å°ç”µè„‘åäº†ï¼Œæ•°æ®è¿˜åœ¨
- å¯ä»¥å­˜å‚¨è¶…å¤§æ–‡ä»¶ï¼ˆPBçº§åˆ«ï¼‰

#### Q2: YARNå’Œæ“ä½œç³»ç»Ÿæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**æ“ä½œç³»ç»Ÿ**ï¼ˆå¦‚Linuxï¼‰ï¼š
- ç®¡ç†å•å°ç”µè„‘çš„èµ„æºï¼ˆCPUã€å†…å­˜ï¼‰
- å†³å®šå“ªä¸ªç¨‹åºå…ˆè¿è¡Œ

**YARN**ï¼š
- ç®¡ç†**å¤šå°ç”µè„‘**çš„èµ„æºï¼ˆæ•´ä¸ªé›†ç¾¤ï¼‰
- å†³å®šå“ªä¸ªä»»åŠ¡åœ¨å“ªå°ç”µè„‘ä¸Šè¿è¡Œ
- å¯ä»¥åŠ¨æ€åˆ†é…èµ„æºï¼Œç”¨å®Œå›æ”¶

#### Q3: MapReduceå’Œæ™®é€šç¨‹åºæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**æ™®é€šç¨‹åº**ï¼š
- åœ¨ä¸€å°ç”µè„‘ä¸Šè¿è¡Œ
- æ•°æ®å¤ªå¤§æ—¶ï¼Œå¤„ç†å¾ˆæ…¢
- ç”µè„‘åäº†ï¼Œç¨‹åºå°±åœäº†

**MapReduce**ï¼š
- åœ¨å¤šå°ç”µè„‘ä¸Š**å¹¶è¡Œ**è¿è¡Œ
- æ•°æ®åˆ†æˆå°å—ï¼Œå¤šå°ç”µè„‘åŒæ—¶å¤„ç†ï¼Œé€Ÿåº¦å¿«
- æŸå°ç”µè„‘åäº†ï¼Œä»»åŠ¡è‡ªåŠ¨åœ¨å…¶ä»–ç”µè„‘ä¸Šé‡æ–°è¿è¡Œ

#### Q4: ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¹ˆå¤šç»„ä»¶ï¼Ÿä¸èƒ½ç®€åŒ–å—ï¼Ÿ

**ç®€åŒ–ç‰ˆæœ¬**ï¼ˆå•æœºï¼‰ï¼š
- ä¸€å°ç”µè„‘å­˜æ•°æ®ã€è¿è¡Œä»»åŠ¡
- **é—®é¢˜**ï¼šæ•°æ®å¤ªå¤§å­˜ä¸ä¸‹ï¼Œå¤„ç†å¤ªæ…¢

**Hadoopç‰ˆæœ¬**ï¼ˆåˆ†å¸ƒå¼ï¼‰ï¼š
- **HDFS**ï¼šå¤šå°ç”µè„‘å­˜æ•°æ®ï¼ˆè§£å†³å­˜å‚¨é—®é¢˜ï¼‰
- **YARN**ï¼šå¤šå°ç”µè„‘è¿è¡Œä»»åŠ¡ï¼ˆè§£å†³èµ„æºç®¡ç†é—®é¢˜ï¼‰
- **MapReduce**ï¼šå¤šå°ç”µè„‘å¹¶è¡Œè®¡ç®—ï¼ˆè§£å†³è®¡ç®—é—®é¢˜ï¼‰

**ç»“è®º**ï¼šæ¯ä¸ªç»„ä»¶è§£å†³ä¸åŒçš„é—®é¢˜ï¼Œç¼ºä¸€ä¸å¯ã€‚

---

### ğŸ“ è®°å¿†å£è¯€

- **HDFS** = **å­˜æ•°æ®**ï¼ˆåˆ†å¸ƒå¼å­˜å‚¨ï¼‰
- **YARN** = **ç®¡èµ„æº**ï¼ˆèµ„æºç®¡ç†ï¼‰
- **MapReduce** = **ç®—æ•°æ®**ï¼ˆå¹¶è¡Œè®¡ç®—ï¼‰

**å®Œæ•´æµç¨‹**ï¼š
```
æ•°æ®å­˜HDFS â†’ YARNåˆ†é…èµ„æº â†’ MapReduceè®¡ç®— â†’ ç»“æœå­˜HDFS
```

---

## æ•…éšœåœºæ™¯æ€»è§ˆ

### ç»¼åˆæ’åºï¼ˆç»å…¸æ€§ + éƒ¨ç½²éš¾åº¦ï¼‰

| æ’å | æ•…éšœåœºæ™¯ | ç»å…¸æ€§ | éƒ¨ç½²éš¾åº¦ | ç»¼åˆè¯„åˆ† | ç»„ä»¶ |
|------|---------|--------|----------|----------|------|
| 1 | **YARN ResourceManageræœªå¯åŠ¨** | â­â­â­â­â­ | â­ | 9.5 | YARN |
| 2 | **MapReduceä»»åŠ¡å› å†…å­˜ä¸è¶³å¤±è´¥** | â­â­â­â­â­ | â­â­ | 9.0 | YARN/MapReduce |
| 3 | **NodeManageræœªå¯åŠ¨å¯¼è‡´ä»»åŠ¡æ— æ³•åˆ†é…** | â­â­â­â­ | â­ | 8.5 | YARN |
| 4 | **YARNé…ç½®é”™è¯¯ï¼ˆResourceManageråœ°å€é”™è¯¯ï¼‰** | â­â­â­â­ | â­â­ | 8.0 | YARN |
| 5 | **MapReduceä»»åŠ¡å› ç£ç›˜ç©ºé—´ä¸è¶³å¤±è´¥** | â­â­â­â­ | â­â­â­ | 7.5 | HDFS/YARN |
| 6 | **Containerå¯åŠ¨å¤±è´¥ï¼ˆç«¯å£å†²çªï¼‰** | â­â­â­ | â­â­ | 7.0 | YARN |
| 7 | **MapReduceä»»åŠ¡å› ç½‘ç»œè¶…æ—¶å¤±è´¥** | â­â­â­ | â­â­â­ | 6.5 | YARN/Network |
| 8 | **YARNé˜Ÿåˆ—èµ„æºä¸è¶³** | â­â­â­ | â­â­â­ | 6.0 | YARN |
| 9 | **MapReduce Shuffleé˜¶æ®µå¤±è´¥** | â­â­â­â­ | â­â­â­â­ | 6.0 | MapReduce |
| 10 | **NodeManagerç£ç›˜ç©ºé—´ä¸è¶³** | â­â­â­ | â­â­â­ | 5.5 | YARN/HDFS |
| 11 | **MapReduceä»»åŠ¡å› æƒé™é—®é¢˜å¤±è´¥** | â­â­â­ | â­â­â­â­ | 5.0 | HDFS/YARN |
| 12 | **YARN ApplicationMasterå¯åŠ¨å¤±è´¥** | â­â­â­ | â­â­â­â­ | 4.5 | YARN |
| 13 | **MapReduceä»»åŠ¡å› æ•°æ®å€¾æ–œå¤±è´¥** | â­â­â­ | â­â­â­â­â­ | 4.0 | MapReduce |
| 14 | **YARN Timeline Serveræ•…éšœ** | â­â­ | â­â­â­ | 3.5 | YARN |
| 15 | **MapReduceä»»åŠ¡å› ä»£ç é”™è¯¯å¤±è´¥** | â­â­ | â­â­â­â­â­ | 2.5 | MapReduce |

**è¯´æ˜**ï¼š
- **ç»å…¸æ€§**ï¼šè¯¥æ•…éšœåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å‡ºç°çš„é¢‘ç‡å’Œé‡è¦æ€§
- **éƒ¨ç½²éš¾åº¦**ï¼š1=æœ€ç®€å•ï¼ˆé…ç½®å³å¯ï¼‰ï¼Œ5=æœ€å¤æ‚ï¼ˆéœ€è¦å¤æ‚è„šæœ¬æˆ–å¤§é‡æ•°æ®ï¼‰
- **ç»¼åˆè¯„åˆ†**ï¼šç»å…¸æ€§ Ã— 2 - éƒ¨ç½²éš¾åº¦ï¼ˆä¼˜å…ˆç»å…¸ä¸”æ˜“éƒ¨ç½²çš„åœºæ™¯ï¼‰

---

## éƒ¨ç½²å‡†å¤‡

### ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨YARNæœåŠ¡

#### 1.1 æ£€æŸ¥ç°æœ‰é…ç½®

**é‡è¦**ï¼šç»è¿‡éªŒè¯ï¼Œä½ çš„é›†ç¾¤**å°šæœªé…ç½®YARN**ã€‚`yarn-site.xml` å’Œ `mapred-site.xml` éƒ½æ˜¯ç©ºé…ç½®æ–‡ä»¶ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
1. **å…ˆé…ç½®YARN**ï¼ˆä¿®æ”¹é…ç½®æ–‡ä»¶ï¼‰
2. **å†å¯åŠ¨YARNæœåŠ¡**

#### 1.2 YARNæ¶æ„ç®€ä»‹

**YARN (Yet Another Resource Negotiator)** æ˜¯Hadoop 2.0+çš„èµ„æºç®¡ç†æ¡†æ¶ï¼š

- **ResourceManager (RM)**ï¼šèµ„æºç®¡ç†å™¨ï¼Œè´Ÿè´£æ•´ä¸ªé›†ç¾¤çš„èµ„æºåˆ†é…
  - è¿è¡Œåœ¨NameNodeå®¹å™¨ï¼ˆ`namenode`ï¼‰
  - Web UIç«¯å£ï¼š8088
  - RPCç«¯å£ï¼š8032
  
- **NodeManager (NM)**ï¼šèŠ‚ç‚¹ç®¡ç†å™¨ï¼Œè´Ÿè´£å•ä¸ªèŠ‚ç‚¹çš„èµ„æºç®¡ç†
  - è¿è¡Œåœ¨æ¯ä¸ªDataNodeå®¹å™¨ï¼ˆ`datanode1`, `datanode2`, `namenode`ï¼‰
  - Web UIç«¯å£ï¼š8042
  - å‘ResourceManageræ±‡æŠ¥èµ„æºä½¿ç”¨æƒ…å†µ

- **ApplicationMaster (AM)**ï¼šåº”ç”¨ä¸»æ§ï¼Œè´Ÿè´£å•ä¸ªåº”ç”¨çš„ä»»åŠ¡è°ƒåº¦
  - ç”±ResourceManagerå¯åŠ¨
  - è¿è¡Œåœ¨æŸä¸ªNodeManagerä¸Š

#### 1.3 å¯åŠ¨YARNæœåŠ¡

```bash
# åœ¨namenodeå®¹å™¨ä¸­å¯åŠ¨ResourceManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon start resourcemanager"'

# åœ¨æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨NodeManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
docker exec datanode2 sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
```

#### 1.4 éªŒè¯YARNå¯åŠ¨

```bash
# æ£€æŸ¥ResourceManagerè¿›ç¨‹
docker exec namenode sh -c 'su - hadoop -c "jps"'
# åº”è¯¥çœ‹åˆ°ï¼šResourceManager

# æ£€æŸ¥NodeManagerè¿›ç¨‹
docker exec datanode1 sh -c 'su - hadoop -c "jps"'
# åº”è¯¥çœ‹åˆ°ï¼šNodeManager

# è®¿é—®ResourceManager Web UI
# http://localhost:8088
```

---

## è¯¦ç»†åœºæ™¯ä¸éƒ¨ç½²æŒ‡å¯¼

### åœºæ™¯1ï¼šYARN ResourceManageræœªå¯åŠ¨ â­â­â­â­â­ (æœ€ç®€å•)

#### æ•…éšœæè¿°
ç”¨æˆ·æäº¤MapReduceä»»åŠ¡æ—¶ï¼Œä»»åŠ¡æ— æ³•æäº¤ï¼ŒæŠ¥é”™"Connection refused"æˆ–"ResourceManager is not available"ã€‚

#### ç»å…¸æ€§ï¼šâ­â­â­â­â­
- æœ€å¸¸è§çš„YARNæ•…éšœ
- ä»»ä½•MapReduceä»»åŠ¡éƒ½éœ€è¦ResourceManager

#### éƒ¨ç½²éš¾åº¦ï¼šâ­
- åªéœ€åœæ­¢ResourceManageræœåŠ¡

#### æ•…éšœæ³¨å…¥æ­¥éª¤

```bash
# 1. åœæ­¢ResourceManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop resourcemanager"'

# 2. éªŒè¯å·²åœæ­¢
docker exec namenode sh -c 'su - hadoop -c "jps"'
# ä¸åº”è¯¥çœ‹åˆ°ResourceManager

# 3. æäº¤ä¸€ä¸ªç®€å•çš„MapReduceä»»åŠ¡ï¼ˆä¼šå¤±è´¥ï¼‰
docker exec namenode sh -c 'su - hadoop -c "yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 2 10"'
```

#### é¢„æœŸé”™è¯¯ä¿¡æ¯
```
Exception in thread "main" java.net.ConnectException: Call From namenode/192.168.80.2 to namenode:8032 failed on connection exception
```

#### è¯Šæ–­è¦ç‚¹
- æ£€æŸ¥ResourceManagerè¿›ç¨‹æ˜¯å¦å­˜åœ¨ï¼ˆ`jps`ï¼‰
- æ£€æŸ¥ResourceManageræ—¥å¿—ï¼š`/usr/local/hadoop/logs/yarn-hadoop-resourcemanager-namenode.log`
- æ£€æŸ¥ç«¯å£8032æ˜¯å¦ç›‘å¬ï¼š`netstat -tlnp | grep 8032`

#### ä¿®å¤æ–¹æ³•
```bash
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon start resourcemanager"'
```

---

### åœºæ™¯2ï¼šMapReduceä»»åŠ¡å› å†…å­˜ä¸è¶³å¤±è´¥ â­â­â­â­â­

#### æ•…éšœæè¿°
MapReduceä»»åŠ¡è¿è¡Œæ—¶ï¼ŒContainerå› å†…å­˜ä¸è¶³è¢«YARNæ€æ­»ï¼Œä»»åŠ¡å¤±è´¥ã€‚

#### ç»å…¸æ€§ï¼šâ­â­â­â­â­
- ç”Ÿäº§ç¯å¢ƒæœ€å¸¸è§çš„ä»»åŠ¡å¤±è´¥åŸå› 
- èµ„æºåˆ†é…ä¸å½“å¯¼è‡´

#### éƒ¨ç½²éš¾åº¦ï¼šâ­â­
- éœ€è¦é…ç½®è¾ƒå°çš„å†…å­˜é™åˆ¶ï¼Œç„¶åæäº¤éœ€è¦æ›´å¤šå†…å­˜çš„ä»»åŠ¡

#### æ•…éšœæ³¨å…¥æ­¥éª¤

```bash
# 1. ä¿®æ”¹yarn-site.xmlï¼Œé™åˆ¶Containeræœ€å¤§å†…å­˜ä¸º128MBï¼ˆå¾ˆå°ï¼‰
docker exec namenode sh -c 'su - hadoop -c "cat >> /usr/local/hadoop/etc/hadoop/yarn-site.xml << EOF
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>128</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>128</value>
  </property>
EOF"'

# 2. é‡å¯ResourceManagerå’ŒNodeManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"'
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'
docker exec datanode2 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'

# 3. æäº¤ä¸€ä¸ªéœ€è¦è¾ƒå¤šå†…å­˜çš„ä»»åŠ¡ï¼ˆä¼šå¤±è´¥ï¼‰
docker exec namenode sh -c 'su - hadoop -c "yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output"'
```

#### é¢„æœŸé”™è¯¯ä¿¡æ¯
```
Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
```

#### è¯Šæ–­è¦ç‚¹
- æŸ¥çœ‹YARN Web UIï¼šhttp://localhost:8088ï¼ŒæŸ¥çœ‹ä»»åŠ¡å¤±è´¥åŸå› 
- æ£€æŸ¥NodeManageræ—¥å¿—ï¼š`/usr/local/hadoop/logs/yarn-hadoop-nodemanager-*.log`
- æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—ï¼š`yarn logs -applicationId <application_id>`

#### ä¿®å¤æ–¹æ³•
```bash
# æ¢å¤åˆç†çš„å†…å­˜é…ç½®ï¼ˆä¾‹å¦‚512MBæˆ–1GBï¼‰
docker exec namenode sh -c 'su - hadoop -c "sed -i \"/yarn.scheduler.maximum-allocation-mb/d\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'
docker exec namenode sh -c 'su - hadoop -c "sed -i \"/yarn.nodemanager.resource.memory-mb/d\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'
# é‡å¯æœåŠ¡
```

---

### åœºæ™¯3ï¼šNodeManageræœªå¯åŠ¨å¯¼è‡´ä»»åŠ¡æ— æ³•åˆ†é… â­â­â­â­

#### æ•…éšœæè¿°
ResourceManagerè¿è¡Œæ­£å¸¸ï¼Œä½†æ‰€æœ‰NodeManageréƒ½æœªå¯åŠ¨ï¼Œå¯¼è‡´æ— æ³•åˆ†é…Containerï¼Œä»»åŠ¡ä¸€ç›´å¤„äºACCEPTEDçŠ¶æ€ã€‚

#### ç»å…¸æ€§ï¼šâ­â­â­â­
- å¸¸è§äºé›†ç¾¤é‡å¯åå¿˜è®°å¯åŠ¨NodeManager

#### éƒ¨ç½²éš¾åº¦ï¼šâ­
- åªéœ€åœæ­¢NodeManager

#### æ•…éšœæ³¨å…¥æ­¥éª¤

```bash
# 1. åœæ­¢æ‰€æœ‰NodeManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop nodemanager"'
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager"'
docker exec datanode2 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager"'

# 2. éªŒè¯å·²åœæ­¢
docker exec namenode sh -c 'su - hadoop -c "jps"'
# ä¸åº”è¯¥çœ‹åˆ°NodeManager

# 3. æäº¤ä»»åŠ¡ï¼ˆä¼šä¸€ç›´ç­‰å¾…ï¼‰
docker exec namenode sh -c 'su - hadoop -c "yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 2 10"'
```

#### é¢„æœŸé”™è¯¯ä¿¡æ¯
- ä»»åŠ¡çŠ¶æ€ä¸€ç›´ä¸º"ACCEPTED"ï¼Œä¸ä¼šè¿›å…¥"RUNNING"
- ResourceManager Web UIæ˜¾ç¤º"0 active nodes"

#### è¯Šæ–­è¦ç‚¹
- æ£€æŸ¥NodeManagerè¿›ç¨‹ï¼š`jps | grep NodeManager`
- æ£€æŸ¥ResourceManager Web UIï¼šhttp://localhost:8088/cluster/nodes
- æŸ¥çœ‹ResourceManageræ—¥å¿—

#### ä¿®å¤æ–¹æ³•
```bash
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
docker exec datanode2 sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
```

---

### åœºæ™¯4ï¼šYARNé…ç½®é”™è¯¯ï¼ˆResourceManageråœ°å€é”™è¯¯ï¼‰ â­â­â­â­

#### æ•…éšœæè¿°
NodeManageré…ç½®çš„ResourceManageråœ°å€é”™è¯¯ï¼Œå¯¼è‡´NodeManageræ— æ³•è¿æ¥åˆ°ResourceManagerã€‚

#### ç»å…¸æ€§ï¼šâ­â­â­â­
- é…ç½®é”™è¯¯æ˜¯å¸¸è§é—®é¢˜

#### éƒ¨ç½²éš¾åº¦ï¼šâ­â­
- éœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶

#### æ•…éšœæ³¨å…¥æ­¥éª¤

```bash
# 1. ä¿®æ”¹yarn-site.xmlï¼Œå°†ResourceManageråœ°å€æ”¹ä¸ºé”™è¯¯çš„
docker exec datanode1 sh -c 'su - hadoop -c "sed -i \"s/<value>namenode<\/value>/<value>wrong-hostname<\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 2. é‡å¯NodeManager
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'

# 3. ç­‰å¾…å‡ ç§’ï¼Œæ£€æŸ¥NodeManageræ—¥å¿—
docker exec datanode1 sh -c 'su - hadoop -c "tail -20 /usr/local/hadoop/logs/yarn-hadoop-nodemanager-datanode1.log"'
```

#### é¢„æœŸé”™è¯¯ä¿¡æ¯
```
java.net.UnknownHostException: wrong-hostname
æˆ–
java.net.ConnectException: Connection refused
```

#### è¯Šæ–­è¦ç‚¹
- æ£€æŸ¥NodeManageræ—¥å¿—ä¸­çš„è¿æ¥é”™è¯¯
- æ£€æŸ¥yarn-site.xmlä¸­çš„`yarn.resourcemanager.hostname`é…ç½®
- æ£€æŸ¥ResourceManager Web UIï¼Œçœ‹è¯¥èŠ‚ç‚¹æ˜¯å¦åœ¨çº¿

#### ä¿®å¤æ–¹æ³•
```bash
# æ¢å¤æ­£ç¡®çš„é…ç½®
docker exec datanode1 sh -c 'su - hadoop -c "sed -i \"s/<value>wrong-hostname<\/value>/<value>namenode<\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'
```

---

### åœºæ™¯5ï¼šMapReduceä»»åŠ¡å› ç£ç›˜ç©ºé—´ä¸è¶³å¤±è´¥ â­â­â­â­

#### æ•…éšœæè¿°
MapReduceä»»åŠ¡è¿è¡Œæ—¶ï¼Œä¸­é—´ç»“æœæˆ–æœ€ç»ˆè¾“å‡ºå†™å…¥HDFSæ—¶ï¼Œå› ç£ç›˜ç©ºé—´ä¸è¶³å¤±è´¥ã€‚

#### ç»å…¸æ€§ï¼šâ­â­â­â­
- ç”Ÿäº§ç¯å¢ƒå¸¸è§é—®é¢˜

#### éƒ¨ç½²éš¾åº¦ï¼šâ­â­â­
- éœ€è¦é™åˆ¶ç£ç›˜ç©ºé—´æˆ–å¡«å……ç£ç›˜

#### æ•…éšœæ³¨å…¥æ­¥éª¤

```bash
# æ–¹æ³•1ï¼šå¡«å……DataNodeç£ç›˜ï¼ˆç®€å•ä½†å±é™©ï¼‰
# åœ¨datanode1ä¸Šåˆ›å»ºå¤§æ–‡ä»¶å æ»¡ç£ç›˜
docker exec datanode1 sh -c 'dd if=/dev/zero of=/tmp/fill_disk bs=1M count=1000 2>/dev/null || true'

# æ–¹æ³•2ï¼šä¿®æ”¹HDFSé…ç½®ï¼Œé™ä½å¯ç”¨ç©ºé—´é˜ˆå€¼ï¼ˆæ›´å®‰å…¨ï¼‰
# åœ¨namenodeä¸Šä¿®æ”¹hdfs-site.xml
docker exec namenode sh -c 'su - hadoop -c "cat >> /usr/local/hadoop/etc/hadoop/hdfs-site.xml << EOF
  <property>
    <name>dfs.datanode.du.reserved</name>
    <value>107374182400</value>
  </property>
EOF"'
# é‡å¯DataNode
docker exec namenode sh -c 'su - hadoop -c "hdfs --daemon stop datanode && hdfs --daemon start datanode"'

# 3. æäº¤ä¸€ä¸ªä¼šäº§ç”Ÿå¤§é‡è¾“å‡ºçš„ä»»åŠ¡
docker exec namenode sh -c 'su - hadoop -c "hdfs dfs -mkdir -p /input && echo \"test data\" | hdfs dfs -put - /input/test.txt"'
docker exec namenode sh -c 'su - hadoop -c "yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input /output"'
```

#### é¢„æœŸé”™è¯¯ä¿¡æ¯
```
java.io.IOException: No space left on device
```

#### è¯Šæ–­è¦ç‚¹
- æ£€æŸ¥DataNodeç£ç›˜ä½¿ç”¨æƒ…å†µï¼š`df -h`
- æ£€æŸ¥HDFSä½¿ç”¨æƒ…å†µï¼š`hdfs dfsadmin -report`
- æŸ¥çœ‹DataNodeæ—¥å¿—

#### ä¿®å¤æ–¹æ³•
```bash
# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
docker exec datanode1 sh -c 'rm -f /tmp/fill_disk'
# æˆ–æ¢å¤é…ç½®
```

---

### åœºæ™¯6-15ï¼šå…¶ä»–æ•…éšœåœºæ™¯

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œå…¶ä»–åœºæ™¯çš„è¯¦ç»†æŒ‡å¯¼å°†åœ¨åç»­è¡¥å……ã€‚ä»¥ä¸‹æ˜¯ç®€è¦è¯´æ˜ï¼š

**åœºæ™¯6ï¼šContainerå¯åŠ¨å¤±è´¥ï¼ˆç«¯å£å†²çªï¼‰**
- ä¿®æ”¹yarn-site.xmlï¼Œè®¾ç½®NodeManagerä½¿ç”¨çš„ç«¯å£ä¸å·²å ç”¨ç«¯å£å†²çª

**åœºæ™¯7ï¼šMapReduceä»»åŠ¡å› ç½‘ç»œè¶…æ—¶å¤±è´¥**
- ä¿®æ”¹yarn-site.xmlï¼Œè®¾ç½®æçŸ­çš„ç½‘ç»œè¶…æ—¶æ—¶é—´

**åœºæ™¯8ï¼šYARNé˜Ÿåˆ—èµ„æºä¸è¶³**
- é…ç½®YARNé˜Ÿåˆ—ï¼Œé™åˆ¶é˜Ÿåˆ—èµ„æºï¼Œæäº¤è¶…è¿‡é™åˆ¶çš„ä»»åŠ¡

**åœºæ™¯9ï¼šMapReduce Shuffleé˜¶æ®µå¤±è´¥**
- ä¿®æ”¹mapred-site.xmlï¼Œé…ç½®é”™è¯¯çš„ShuffleæœåŠ¡

**åœºæ™¯10ï¼šNodeManagerç£ç›˜ç©ºé—´ä¸è¶³**
- ç±»ä¼¼åœºæ™¯5ï¼Œä½†é’ˆå¯¹NodeManageræœ¬åœ°ç›®å½•

**åœºæ™¯11ï¼šMapReduceä»»åŠ¡å› æƒé™é—®é¢˜å¤±è´¥**
- ä¿®æ”¹HDFSç›®å½•æƒé™ï¼Œä½¿ä»»åŠ¡æ— æ³•å†™å…¥

**åœºæ™¯12ï¼šYARN ApplicationMasterå¯åŠ¨å¤±è´¥**
- é…ç½®é”™è¯¯å¯¼è‡´AMæ— æ³•å¯åŠ¨

**åœºæ™¯13ï¼šMapReduceä»»åŠ¡å› æ•°æ®å€¾æ–œå¤±è´¥**
- å‡†å¤‡å€¾æ–œæ•°æ®ï¼Œæäº¤ä»»åŠ¡

**åœºæ™¯14ï¼šYARN Timeline Serveræ•…éšœ**
- Timeline Serveræœªå¯åŠ¨æˆ–é…ç½®é”™è¯¯

**åœºæ™¯15ï¼šMapReduceä»»åŠ¡å› ä»£ç é”™è¯¯å¤±è´¥**
- ç¼–å†™æœ‰bugçš„MapReduceç¨‹åº

---

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### æ¨èé¡ºåº

1. **ç«‹å³å¼€å§‹**ï¼šåœºæ™¯1ï¼ˆResourceManageræœªå¯åŠ¨ï¼‰- æœ€ç®€å•ï¼ŒéªŒè¯YARNåŸºç¡€åŠŸèƒ½
2. **ç¬¬äºŒä¼˜å…ˆçº§**ï¼šåœºæ™¯2ï¼ˆå†…å­˜ä¸è¶³ï¼‰- ç»å…¸ä¸”å®ç”¨
3. **ç¬¬ä¸‰ä¼˜å…ˆçº§**ï¼šåœºæ™¯3ï¼ˆNodeManageræœªå¯åŠ¨ï¼‰- å®Œå–„YARNè¯Šæ–­èƒ½åŠ›
4. **åç»­æ‰©å±•**ï¼šæ ¹æ®ä½ çš„éœ€æ±‚ï¼Œé€æ­¥æ·»åŠ å…¶ä»–åœºæ™¯

### éœ€è¦æˆ‘å¸®ä½ åšä»€ä¹ˆï¼Ÿ

1. **ç”Ÿæˆå®Œæ•´çš„YARNå¯åŠ¨è„šæœ¬**ï¼šä¸€é”®å¯åŠ¨/åœæ­¢YARNæœåŠ¡
2. **ç”Ÿæˆæ•…éšœæ³¨å…¥è„šæœ¬**ï¼šè‡ªåŠ¨åŒ–æ³¨å…¥ä¸Šè¿°æ•…éšœ
3. **æ‰©å±•è¯Šæ–­ç³»ç»Ÿ**ï¼šæ·»åŠ YARNç›¸å…³çš„è¯Šæ–­å·¥å…·å’ŒçŸ¥è¯†åº“
4. **å‡†å¤‡æµ‹è¯•æ•°æ®**ï¼šä¸ºMapReduceä»»åŠ¡å‡†å¤‡è¾“å…¥æ•°æ®

å‘Šè¯‰æˆ‘ä½ æƒ³ä»å“ªä¸ªåœºæ™¯å¼€å§‹ï¼Œæˆ‘ä¼šæä¾›è¯¦ç»†çš„æ­¥éª¤æŒ‡å¯¼ï¼

