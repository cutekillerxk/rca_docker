# 故障复现与记录方案

## 目标

系统性地复现 `FAULT_TYPE_LIBRARY` 中定义的10个故障类型，并记录：
1. 故障注入过程
2. 故障时的集群日志
3. Agent的诊断结果
4. 标准答案（ground truth）

## 工作流程

```
1. 准备阶段
   ├── 确保集群正常运行
   ├── 备份当前状态
   └── 重置日志读取器状态

2. 故障注入
   ├── 执行故障注入操作（停止服务、修改配置等）
   ├── 等待故障生效（通常5-10秒）
   └── 验证故障已生效

3. 日志收集
   ├── 收集所有节点日志
   ├── 收集JMX监控指标
   └── 执行诊断命令（hdfs dfsadmin -report等）

4. Agent诊断
   ├── 使用收集的日志作为输入
   ├── 调用Agent进行诊断
   └── 记录诊断结果

5. 结果记录
   ├── 保存日志到 cluster_logs.txt
   ├── 保存诊断结果到 diagnosis_result.json
   ├── 创建 ground_truth.json
   └── 更新 metadata.json

6. 恢复阶段
   ├── 恢复集群到正常状态
   └── 验证集群已恢复
```

## 故障复现方法

### HDFS故障

#### 1. datanode_down (DataNode下线)
**复现方法**（模拟节点意外下线）：
```bash
# 方法1：强制杀死DataNode进程（推荐，模拟进程崩溃）
docker exec datanode1 sh -c 'su - hadoop -c "pkill -9 -f DataNode"'
# 或者使用kill命令
# docker exec datanode1 sh -c 'su - hadoop -c "kill -9 \$(jps | grep DataNode | awk \"{print \\\$1}\")"'

# 方法2：停止容器（模拟容器崩溃，更彻底）
# docker stop datanode1
```

**恢复方法**：
```bash
# 启动DataNode服务
docker exec datanode1 sh -c 'su - hadoop -c "hdfs --daemon start datanode"'
# 如果容器被停止，先启动容器
# docker start datanode1
# docker exec datanode1 sh -c 'su - hadoop -c "hdfs --daemon start datanode"'
```

**说明**：
- 使用 `pkill -9` 或 `kill -9` 强制终止进程，模拟进程崩溃场景
- 使用 `hdfs --daemon stop` 是正常关闭，不会产生"意外下线"的故障特征

#### 2. namenode_safemode (NameNode安全模式)
**复现方法**：
```bash
# 方法1：停止部分DataNode，触发安全模式
docker exec datanode1 sh -c 'su - hadoop -c "hdfs --daemon stop datanode"'
docker exec datanode2 sh -c 'su - hadoop -c "hdfs --daemon stop datanode"'
# 等待30秒，NameNode可能进入安全模式

# 方法2：手动进入安全模式（更可控）
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode enter"'
```

**恢复方法**：
```bash
# 退出安全模式
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode leave"'
# 恢复DataNode
docker exec datanode1 sh -c 'su - hadoop -c "hdfs --daemon start datanode"'
docker exec datanode2 sh -c 'su - hadoop -c "hdfs --daemon start datanode"'
```

#### 3. cluster_id_mismatch (集群ID不匹配)
**复现方法**：
```bash
# 1. 停止集群
docker exec namenode sh -c 'su - hadoop -c "stop-dfs.sh"'

# 2. 备份并删除DataNode的VERSION文件
docker exec datanode1 sh -c 'su - hadoop -c "rm -rf /usr/local/hadoop/hdfs/datanode/current/VERSION"'
docker exec datanode2 sh -c 'su - hadoop -c "rm -rf /usr/local/hadoop/hdfs/datanode/current/VERSION"'

# 3. 重新格式化NameNode（生成新的clusterID）
docker exec namenode sh -c 'su - hadoop -c "hdfs namenode -format"'

# 4. 启动集群
docker exec namenode sh -c 'su - hadoop -c "start-dfs.sh"'
```

**恢复方法**：
```bash
# 1. 停止集群
docker exec namenode sh -c 'su - hadoop -c "stop-dfs.sh"'

# 2. 清理DataNode元数据
docker exec datanode1 sh -c 'su - hadoop -c "rm -rf /usr/local/hadoop/hdfs/datanode/current/*"'
docker exec datanode2 sh -c 'su - hadoop -c "rm -rf /usr/local/hadoop/hdfs/datanode/current/*"'

# 3. 启动集群（DataNode会自动获取新的clusterID）
docker exec namenode sh -c 'su - hadoop -c "start-dfs.sh"'
```

### YARN故障

#### 4. resourcemanager_down (ResourceManager下线)
**复现方法**（模拟进程崩溃）：
```bash
# 强制杀死ResourceManager进程
docker exec namenode sh -c 'su - hadoop -c "pkill -9 -f ResourceManager"'
# 或者
# docker exec namenode sh -c 'su - hadoop -c "kill -9 \$(jps | grep ResourceManager | awk \"{print \\\$1}\")"'
```

**恢复方法**：
```bash
# 启动ResourceManager服务
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon start resourcemanager"'
```

#### 5. nodemanager_down (NodeManager下线)
**复现方法**（模拟进程崩溃）：
```bash
# 强制杀死NodeManager进程（在datanode1）
docker exec datanode1 sh -c 'su - hadoop -c "pkill -9 -f NodeManager"'
# 或者
# docker exec datanode1 sh -c 'su - hadoop -c "kill -9 \$(jps | grep NodeManager | awk \"{print \\\$1}\")"'
```

**恢复方法**：
```bash
# 启动NodeManager服务
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
```

#### 6. yarn_config_error (YARN配置错误)
**复现方法**：
```bash
# 1. 备份yarn-site.xml
docker exec datanode1 sh -c 'su - hadoop -c "cp /usr/local/hadoop/etc/hadoop/yarn-site.xml /usr/local/hadoop/etc/hadoop/yarn-site.xml.bak"'

# 2. 修改ResourceManager地址为错误值
docker exec datanode1 sh -c 'su - hadoop -c "sed -i \"s/<value>namenode<\\/value>/<value>wrong-hostname<\\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 3. 重启NodeManager
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'
```


**恢复方法**：
```bash
# 恢复配置文件
docker exec datanode1 sh -c 'su - hadoop -c "cp /usr/local/hadoop/etc/hadoop/yarn-site.xml.bak /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 重启NodeManager
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager && yarn --daemon start nodemanager"'
```

### MapReduce故障

#### 7. mapreduce_memory_insufficient (MapReduce任务内存不足)
**复现方法**：
```bash
# 1. 修改YARN内存配置为很小的值
docker exec namenode sh -c 'su - hadoop -c "sed -i \"s/<value>2048<\\/value>/<value>128<\\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 2. 重启ResourceManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"'

# 3. 提交一个需要大量内存的MapReduce任务
# （需要准备测试任务）
```

**恢复方法**：
```bash
# 恢复内存配置
docker exec namenode sh -c 'su - hadoop -c "sed -i \"s/<value>128<\\/value>/<value>2048<\\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 重启ResourceManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"'
```

#### 8. mapreduce_disk_insufficient (MapReduce任务磁盘空间不足)
**复现方法**：
```bash
# 1. 填充磁盘空间（创建大文件）
docker exec datanode1 sh -c 'dd if=/dev/zero of=/tmp/large_file bs=1M count=1000'

# 2. 提交MapReduce任务（会因磁盘空间不足失败）
```

**恢复方法**：
```bash
# 删除大文件
docker exec datanode1 sh -c 'rm -f /tmp/large_file'
```

#### 9. mapreduce_shuffle_failed (MapReduce Shuffle阶段失败)
**复现方法**：
```bash
# 1. 停止Shuffle服务（通过停止NodeManager）
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon stop nodemanager"'

# 2. 提交MapReduce任务（Shuffle阶段会失败）
```

**恢复方法**：
```bash
# 启动NodeManager
docker exec datanode1 sh -c 'su - hadoop -c "yarn --daemon start nodemanager"'
```

#### 10. mapreduce_task_timeout (MapReduce任务超时)
**复现方法**：
```bash
# 1. 修改超时配置为很短的值
docker exec namenode sh -c 'su - hadoop -c "sed -i \"s/<value>600000<\\/value>/<value>1000<\\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 2. 重启ResourceManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"'

# 3. 提交一个需要较长时间的任务
```

**恢复方法**：
```bash
# 恢复超时配置
docker exec namenode sh -c 'su - hadoop -c "sed -i \"s/<value>1000<\\/value>/<value>600000<\\/value>/\" /usr/local/hadoop/etc/hadoop/yarn-site.xml"'

# 重启ResourceManager
docker exec namenode sh -c 'su - hadoop -c "yarn --daemon stop resourcemanager && yarn --daemon start resourcemanager"'
```

## 注意事项

1. **安全性**：
   - 在测试环境中进行，不要在生产环境操作
   - 每次故障复现后必须恢复集群
   - 某些故障（如cluster_id_mismatch）会丢失数据，需要备份

2. **顺序**：
   - 建议按故障类型顺序进行，避免相互影响
   - 每个故障复现后，等待足够时间让故障生效（5-10秒）

3. **验证**：
   - 故障注入后，验证故障确实已生效
   - 使用 `hdfs dfsadmin -report`、`jps` 等命令验证

4. **日志收集时机**：
   - 在故障生效后立即收集日志
   - 确保日志包含故障相关的错误信息

5. **恢复验证**：
   - 恢复后验证集群已恢复正常
   - 检查服务状态、监控指标等

## 自动化脚本

使用 `reproduce_fault.py` 脚本可以自动化执行故障复现、日志收集和诊断过程。
