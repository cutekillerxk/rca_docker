# æ•…éšœå¤çŽ°æ“ä½œæŒ‡å—

## æ€»ä½“æµç¨‹

```
1. å‡†å¤‡é˜¶æ®µ â†’ 2. æ•…éšœæ³¨å…¥ â†’ 3. æ—¥å¿—æ”¶é›† â†’ 4. Agentè¯Šæ–­ â†’ 5. ç»“æžœæ•´ç† â†’ 6. æ¢å¤é›†ç¾¤
```

## è¯¦ç»†æ­¥éª¤

### æ­¥éª¤1ï¼šå‡†å¤‡é˜¶æ®µ

**æ£€æŸ¥é›†ç¾¤çŠ¶æ€**
```bash
# æ£€æŸ¥HDFSé›†ç¾¤çŠ¶æ€
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"'

# æ£€æŸ¥Javaè¿›ç¨‹ï¼ˆç¡®è®¤æœåŠ¡è¿è¡Œï¼‰
docker exec namenode sh -c 'su - hadoop -c "jps"' 
docker exec datanode1 sh -c 'su - hadoop -c "jps"'
docker exec datanode2 sh -c 'su - hadoop -c "jps"'
```

**å‡†å¤‡è®°å½•ç›®å½•**
```bash
# åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ç›®å½•ï¼ˆä»¥datanode_downä¸ºä¾‹ï¼‰
mkdir -p test_cases/hdfs/datanode_down/case1
cd test_cases/hdfs/datanode_down/case1
```

**è®°å½•å½“å‰æ—¶é—´**
- æ•…éšœæ³¨å…¥æ—¶é—´ï¼š________
- é¢„è®¡æ¢å¤æ—¶é—´ï¼š________

---

### æ­¥éª¤2ï¼šæ•…éšœæ³¨å…¥

æ ¹æ®è¦å¤çŽ°çš„æ•…éšœç±»åž‹ï¼Œå‚è€ƒ `æ•…éšœå¤çŽ°æ–¹æ¡ˆ.md` æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚

#### ç¤ºä¾‹1ï¼šDataNodeä¸‹çº¿ (datanode_down)

**é‡è¦**ï¼šè¦æ¨¡æ‹Ÿ"èŠ‚ç‚¹æ„å¤–ä¸‹çº¿"æ•…éšœï¼Œåº”è¯¥ä½¿ç”¨å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹çš„æ–¹å¼ï¼Œè€Œä¸æ˜¯æ­£å¸¸å…³é—­ã€‚

```bash
# æ–¹æ³•1ï¼šå¼ºåˆ¶æ€æ­»DataNodeè¿›ç¨‹ï¼ˆæŽ¨èï¼Œæ¨¡æ‹Ÿè¿›ç¨‹å´©æºƒï¼‰
docker exec datanode1 sh -c 'su - hadoop -c "pkill -9 -f DataNode"'
# æˆ–è€…ä½¿ç”¨killå‘½ä»¤
# docker exec datanode1 sh -c 'su - hadoop -c "kill -9 \$(jps | grep DataNode | awk \"{print \\\$1}\")"'

# æ–¹æ³•2ï¼šåœæ­¢å®¹å™¨ï¼ˆæ¨¡æ‹Ÿå®¹å™¨å´©æºƒï¼Œæ›´å½»åº•ï¼‰
# docker stop datanode1

# ç­‰å¾…5-10ç§’è®©æ•…éšœç”Ÿæ•ˆ
sleep 5

# éªŒè¯æ•…éšœå·²ç”Ÿæ•ˆ
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"'
# åº”è¯¥çœ‹åˆ°ï¼šDead datanodes: 1

# ç¡®è®¤è¿›ç¨‹å·²åœæ­¢
docker exec datanode1 sh -c 'su - hadoop -c "jps"'
# åº”è¯¥çœ‹ä¸åˆ° DataNode è¿›ç¨‹
```

**è¯´æ˜Ž**ï¼š
- `pkill -9 -f DataNode`ï¼šå¼ºåˆ¶æ€æ­»åŒ…å«"DataNode"çš„è¿›ç¨‹ï¼Œæ¨¡æ‹Ÿè¿›ç¨‹å´©æºƒ
- `kill -9`ï¼šå‘é€SIGKILLä¿¡å·ï¼Œè¿›ç¨‹æ— æ³•æ•èŽ·ï¼Œç«‹å³ç»ˆæ­¢
- ä½¿ç”¨ `hdfs --daemon stop` æ˜¯æ­£å¸¸å…³é—­ï¼Œä¸ä¼šäº§ç”Ÿ"æ„å¤–ä¸‹çº¿"çš„æ•…éšœåœºæ™¯

#### ç¤ºä¾‹2ï¼šNameNodeå®‰å…¨æ¨¡å¼ (namenode_safemode)

```bash
# è¿›å…¥å®‰å…¨æ¨¡å¼
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode enter"'

# ç­‰å¾…3ç§’
sleep 3

# éªŒè¯æ•…éšœå·²ç”Ÿæ•ˆ
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode get"'
# åº”è¯¥çœ‹åˆ°ï¼šSafe mode is ON
```

#### ç¤ºä¾‹3ï¼šResourceManagerä¸‹çº¿ (resourcemanager_down)

```bash
# å¼ºåˆ¶æ€æ­»ResourceManagerè¿›ç¨‹ï¼ˆæ¨¡æ‹Ÿè¿›ç¨‹å´©æºƒï¼‰
docker exec namenode sh -c 'su - hadoop -c "pkill -9 -f ResourceManager"'
# æˆ–è€…ä½¿ç”¨killå‘½ä»¤ï¼ˆéœ€è¦å…ˆèŽ·å–PIDï¼‰
# docker exec namenode sh -c 'su - hadoop -c "kill -9 \$(jps | grep ResourceManager | awk \"{print \\\$1}\")"'

# ç­‰å¾…3ç§’
sleep 3

# éªŒè¯æ•…éšœå·²ç”Ÿæ•ˆ
docker exec namenode sh -c 'su - hadoop -c "jps"'
# åº”è¯¥çœ‹ä¸åˆ° ResourceManager è¿›ç¨‹
```

#### ç¤ºä¾‹4ï¼šNodeManagerä¸‹çº¿ (nodemanager_down)

```bash
# å¼ºåˆ¶æ€æ­»NodeManagerè¿›ç¨‹ï¼ˆåœ¨datanode1ï¼‰
docker exec datanode1 sh -c 'su - hadoop -c "pkill -9 -f NodeManager"'
# æˆ–è€…
# docker exec datanode1 sh -c 'su - hadoop -c "kill -9 \$(jps | grep NodeManager | awk \"{print \\\$1}\")"'

# ç­‰å¾…3ç§’
sleep 3

# éªŒè¯æ•…éšœå·²ç”Ÿæ•ˆ
docker exec datanode1 sh -c 'su - hadoop -c "jps"'
# åº”è¯¥çœ‹ä¸åˆ° NodeManager è¿›ç¨‹
```

**æ›´å¤šæ•…éšœç±»åž‹çš„å¤çŽ°æ–¹æ³•ï¼Œè¯·å‚è€ƒ `æ•…éšœå¤çŽ°æ–¹æ¡ˆ.md`**

---

### æ­¥éª¤3ï¼šæ—¥å¿—æ”¶é›†

**æ–¹æ³•1ï¼šä½¿ç”¨çŽ°æœ‰çš„æ—¥å¿—æ”¶é›†å·¥å…·ï¼ˆæŽ¨èï¼‰**

åœ¨Gradioç•Œé¢æˆ–é€šè¿‡Agentå·¥å…·è°ƒç”¨ `get_cluster_logs`ï¼ŒèŽ·å–é›†ç¾¤æ—¥å¿—ã€‚

**æ–¹æ³•2ï¼šæ‰‹åŠ¨æ”¶é›†æ—¥å¿—**

```bash
# æ”¶é›†NameNodeæ—¥å¿—
docker exec namenode tail -200 /usr/local/hadoop/logs/hadoop-hadoop-namenode-namenode.log > namenode.log

# æ”¶é›†DataNodeæ—¥å¿—
docker exec datanode1 tail -200 /usr/local/hadoop/logs/hadoop-hadoop-datanode-datanode1.log > datanode1.log
docker exec datanode2 tail -200 /usr/local/hadoop/logs/hadoop-hadoop-datanode-datanode2.log > datanode2.log

# æ”¶é›†é›†ç¾¤çŠ¶æ€
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"' > cluster_report.txt
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode get"' > safemode_status.txt

# æ”¶é›†è¿›ç¨‹çŠ¶æ€
docker exec namenode sh -c 'su - hadoop -c "jps"' > jps_namenode.txt
docker exec datanode1 sh -c 'su - hadoop -c "jps"' > jps_datanode1.txt
docker exec datanode2 sh -c 'su - hadoop -c "jps"' > jps_datanode2.txt
```

**æ•´ç†æ—¥å¿—æ–‡ä»¶**

å°†æ‰€æœ‰æ”¶é›†çš„æ—¥å¿—æ•´ç†åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œä¿å­˜ä¸º `cluster_logs.txt`ï¼š

```bash
cat > cluster_logs.txt << 'EOF'
[é›†ç¾¤æ—¥å¿—åˆ†æžä»»åŠ¡]
å…±å‘çŽ° X ä¸ªèŠ‚ç‚¹éœ€è¦åˆ†æžã€‚
è¯·é€ä¸ªåˆ†æžæ¯ä¸ªèŠ‚ç‚¹çš„æ—¥å¿—ï¼Œæ¯ä¸ªèŠ‚ç‚¹åˆ†æžä¸€æ¬¡ã€‚

=== NameNode ===
[ç²˜è´´NameNodeæ—¥å¿—å†…å®¹]

=== DataNode1 ===
[ç²˜è´´DataNode1æ—¥å¿—å†…å®¹]

=== DataNode2 ===
[ç²˜è´´DataNode2æ—¥å¿—å†…å®¹]

=== é›†ç¾¤çŠ¶æ€ ===
[ç²˜è´´é›†ç¾¤çŠ¶æ€æŠ¥å‘Š]

=== è¿›ç¨‹çŠ¶æ€ ===
[ç²˜è´´è¿›ç¨‹çŠ¶æ€]
EOF
```

**ä¿å­˜åˆ°æµ‹è¯•ç”¨ä¾‹ç›®å½•**

```bash
# å°† cluster_logs.txt ä¿å­˜åˆ°æµ‹è¯•ç”¨ä¾‹ç›®å½•
cp cluster_logs.txt test_cases/hdfs/datanode_down/case1/
```

---

### æ­¥éª¤4ï¼šAgentè¯Šæ–­

**æ–¹æ³•1ï¼šä½¿ç”¨Gradioç•Œé¢ï¼ˆæŽ¨èï¼‰**

1. æ‰“å¼€Gradioç•Œé¢
2. è¾“å…¥è¯Šæ–­è¯·æ±‚ï¼š
   ```
   è¯·åˆ†æžä»¥ä¸‹é›†ç¾¤æ—¥å¿—ï¼Œè¯Šæ–­é›†ç¾¤çŠ¶æ€ï¼š
   
   [ç²˜è´´ cluster_logs.txt çš„å†…å®¹]
   ```
3. èŽ·å–è¯Šæ–­ç»“æžœ
4. å¤åˆ¶è¯Šæ–­ç»“æžœï¼Œä¿å­˜åˆ° `return.txt`

**æ–¹æ³•2ï¼šé€šè¿‡Pythonè°ƒç”¨Agent**

```python
from cl_agent.agent import create_agent_instance

# è¯»å–æ—¥å¿—
with open('cluster_logs.txt', 'r', encoding='utf-8') as f:
    cluster_logs = f.read()

# åˆ›å»ºAgent
agent = create_agent_instance()

# è¯Šæ–­
request = f"è¯·åˆ†æžä»¥ä¸‹é›†ç¾¤æ—¥å¿—ï¼Œè¯Šæ–­é›†ç¾¤çŠ¶æ€ï¼š\n\n{cluster_logs}"
result = agent.invoke({"messages": [("user", request)]})

# æå–ç»“æžœ
diagnosis_result = result["messages"][-1].content

# ä¿å­˜ä¸ºçº¯æ–‡æœ¬ï¼ˆå¯¹åº”Datasetçš„outputï¼‰
with open('return.txt', 'w', encoding='utf-8') as f:
    f.write(diagnosis_result)
```

**æ³¨æ„**ï¼š
- `return.txt` åº”è¯¥åŒ…å«Agentçš„å®Œæ•´è¯Šæ–­ç»“æžœ
- æ ¼å¼åº”è¯¥æ˜¯å¯¹è¯é£Žæ ¼çš„æ–‡æœ¬ï¼Œç¬¦åˆSystem Promptçš„è¾“å‡ºè¦æ±‚
- è¿™ä¸ªæ–‡ä»¶å¯¹åº”Datasetä¸­çš„ `output` å­—æ®µ

---

### æ­¥éª¤5ï¼šç»“æžœæ•´ç†

**ä¿å­˜Agentè¯Šæ–­ç»“æžœ**

å°†Agentçš„è¯Šæ–­ç»“æžœä¿å­˜ä¸º `return.txt`ï¼š

```bash
# æ–¹æ³•1ï¼šä»ŽGradioç•Œé¢å¤åˆ¶è¯Šæ–­ç»“æžœï¼Œä¿å­˜åˆ°æ–‡ä»¶
cat > return.txt << 'EOF'
[ç²˜è´´Agentçš„è¯Šæ–­ç»“æžœ]
EOF

# æ–¹æ³•2ï¼šå¦‚æžœä½¿ç”¨Pythonè°ƒç”¨Agentï¼Œç›´æŽ¥ä¿å­˜ç»“æžœ
# å°†è¯Šæ–­ç»“æžœä¿å­˜ä¸ºçº¯æ–‡æœ¬æ ¼å¼
```

**return.txt æ ¼å¼è¦æ±‚**ï¼š
- å¯¹è¯é£Žæ ¼çš„æ–‡æœ¬
- åŒ…å«ï¼šæ•´ä½“çŠ¶æ€ã€è¯Šæ–­æ‘˜è¦ã€æ•…éšœè¯¦æƒ…
- æ•…éšœè¯¦æƒ…åŒ…å«ï¼šæ•…éšœåç§°ã€ç½®ä¿¡åº¦ã€å—å½±å“èŠ‚ç‚¹ã€ç—‡çŠ¶ã€æ ¹æœ¬åŽŸå› ã€è¯æ®ã€ä¿®å¤æ­¥éª¤
- æ ¼å¼ç¬¦åˆSystem Promptçš„è¾“å‡ºè¦æ±‚ï¼ˆå‚è€ƒDatasetä¸­çš„outputæ ¼å¼ï¼‰

**æ–‡ä»¶æ¸…å•æ£€æŸ¥**

ç¡®ä¿æµ‹è¯•ç”¨ä¾‹ç›®å½•åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
test_cases/hdfs/datanode_down/case1/
â”œâ”€â”€ cluster_logs.txt          # âœ… æ•…éšœæ—¶çš„é›†ç¾¤æ—¥å¿—ï¼ˆå¯¹åº”Datasetçš„inputï¼‰
â””â”€â”€ return.txt               # âœ… Agentçš„è¯Šæ–­ç»“æžœï¼ˆå¯¹åº”Datasetçš„outputï¼‰
```

**è¯´æ˜Ž**ï¼š
- `cluster_logs.txt`ï¼šæ•…éšœæ—¶çš„å®Œæ•´é›†ç¾¤æ—¥å¿—ï¼Œæ ¼å¼ä¸Ž `get_cluster_logs()` å·¥å…·è¿”å›žçš„æ ¼å¼ä¸€è‡´
- `return.txt`ï¼šAgentçš„è¯Šæ–­ç»“æžœï¼Œå¯¹è¯é£Žæ ¼çš„æ–‡æœ¬ï¼Œæ ¼å¼ç¬¦åˆSystem Promptè¦æ±‚
- è¿™ä¸¤ä¸ªæ–‡ä»¶å¯ä»¥ç›´æŽ¥ç”¨äºŽç”ŸæˆDatasetæ ·æœ¬ï¼ˆinstruction + input + outputï¼‰

---

### æ­¥éª¤6ï¼šæ¢å¤é›†ç¾¤

æ ¹æ® `æ•…éšœå¤çŽ°æ–¹æ¡ˆ.md` ä¸­çš„æ¢å¤æ–¹æ³•æ‰§è¡Œæ¢å¤æ“ä½œã€‚

#### ç¤ºä¾‹1ï¼šæ¢å¤DataNode

```bash
# å¦‚æžœè¿›ç¨‹è¢«killï¼Œç›´æŽ¥å¯åŠ¨æœåŠ¡
docker exec datanode1 sh -c 'su - hadoop -c "hdfs --daemon start datanode"'

# å¦‚æžœå®¹å™¨è¢«åœæ­¢ï¼Œå…ˆå¯åŠ¨å®¹å™¨
# docker start datanode1
# docker exec datanode1 sh -c 'su - hadoop -c "hdfs --daemon start datanode"'

# ç­‰å¾…5-10ç§’
sleep 5

# éªŒè¯æ¢å¤
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"'
# åº”è¯¥çœ‹åˆ°ï¼šDead datanodes: 0

# ç¡®è®¤è¿›ç¨‹å·²å¯åŠ¨
docker exec datanode1 sh -c 'su - hadoop -c "jps"'
# åº”è¯¥çœ‹åˆ° DataNode è¿›ç¨‹
```

#### ç¤ºä¾‹2ï¼šé€€å‡ºå®‰å…¨æ¨¡å¼

```bash
# é€€å‡ºå®‰å…¨æ¨¡å¼
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode leave"'

# éªŒè¯æ¢å¤
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode get"'
# åº”è¯¥çœ‹åˆ°ï¼šSafe mode is OFF
```

**éªŒè¯é›†ç¾¤å®Œå…¨æ¢å¤**

```bash
# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"'

# æ£€æŸ¥æ‰€æœ‰æœåŠ¡è¿›ç¨‹
docker exec namenode sh -c 'su - hadoop -c "jps"'
docker exec datanode1 sh -c 'su - hadoop -c "jps"'
docker exec datanode2 sh -c 'su - hadoop -c "jps"'
```

---

## æ•…éšœå¤çŽ°é¡ºåºå»ºè®®

### ç¬¬ä¸€è½®ï¼šç®€å•æ•…éšœï¼ˆå…ˆç»ƒä¹ ï¼‰

1. **datanode_down** - DataNodeä¸‹çº¿
   - æ“ä½œç®€å•ï¼Œæ¢å¤å®¹æ˜“
   - å½±å“èŒƒå›´æ˜Žç¡®

2. **namenode_safemode** - NameNodeå®‰å…¨æ¨¡å¼
   - æ“ä½œç®€å•ï¼Œæ¢å¤å®¹æ˜“
   - ä¸å½±å“æ•°æ®

3. **resourcemanager_down** - ResourceManagerä¸‹çº¿
   - æ“ä½œç®€å•ï¼Œæ¢å¤å®¹æ˜“
   - åªå½±å“YARN

4. **nodemanager_down** - NodeManagerä¸‹çº¿
   - æ“ä½œç®€å•ï¼Œæ¢å¤å®¹æ˜“
   - åªå½±å“å•ä¸ªèŠ‚ç‚¹

### ç¬¬äºŒè½®ï¼šé…ç½®æ•…éšœ

5. **yarn_config_error** - YARNé…ç½®é”™è¯¯
   - éœ€è¦ä¿®æ”¹é…ç½®æ–‡ä»¶
   - è®°å¾—å¤‡ä»½å’Œæ¢å¤é…ç½®

### ç¬¬ä¸‰è½®ï¼šå¤æ‚æ•…éšœï¼ˆæœ€åŽè¿›è¡Œï¼‰

6. **cluster_id_mismatch** - é›†ç¾¤IDä¸åŒ¹é…
   - âš ï¸ **ç ´åæ€§æ“ä½œ**ï¼Œä¼šä¸¢å¤±æ•°æ®
   - éœ€è¦é‡æ–°æ ¼å¼åŒ–NameNode
   - å»ºè®®æœ€åŽè¿›è¡Œ

### ç¬¬å››è½®ï¼šMapReduceæ•…éšœï¼ˆéœ€è¦æµ‹è¯•ä»»åŠ¡ï¼‰

7-10. MapReduceç›¸å…³æ•…éšœ
   - éœ€è¦å…ˆå‡†å¤‡MapReduceæµ‹è¯•ä»»åŠ¡
   - æŒ‰éœ€å¤çŽ°

---

## è®°å½•æ¨¡æ¿

### æ•…éšœå¤çŽ°è®°å½•è¡¨

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **æ•…éšœç±»åž‹** | datanode_down |
| **æ•…éšœåç§°** | DataNodeä¸‹çº¿ |
| **å¤çŽ°æ—¶é—´** | 2026-01-23 18:00:00 |
| **å¤çŽ°æ–¹æ³•** | åœæ­¢DataNode1æœåŠ¡ |
| **æ•…éšœç—‡çŠ¶** | Dead datanodes: 1 |
| **æ—¥å¿—æ–‡ä»¶** | cluster_logs.txt |
| **è¯Šæ–­ç»“æžœ** | return.txt |
| **æ¢å¤æ–¹æ³•** | å¯åŠ¨DataNode1æœåŠ¡ |
| **æ¢å¤æ—¶é—´** | 2026-01-23 18:05:00 |
| **æ¢å¤éªŒè¯** | Dead datanodes: 0 âœ… |
| **å¤‡æ³¨** | æ—  |

---

## æ³¨æ„äº‹é¡¹

### 1. å®‰å…¨æ€§

- âœ… **ä»…åœ¨æµ‹è¯•çŽ¯å¢ƒæ“ä½œ**
- âœ… **ä¸è¦åœ¨ç”Ÿäº§çŽ¯å¢ƒæ‰§è¡Œ**
- âš ï¸ **cluster_id_mismatch ä¼šç ´åé›†ç¾¤ï¼Œéœ€è¦é‡æ–°æ ¼å¼åŒ–**
- âš ï¸ **æŸäº›æ“ä½œå¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œè¯·å…ˆå¤‡ä»½**

### 2. æ—¶é—´æŽ§åˆ¶

- **æ•…éšœæ³¨å…¥åŽ**ï¼šç­‰å¾…5-10ç§’è®©æ•…éšœç”Ÿæ•ˆ
- **æ—¥å¿—æ”¶é›†**ï¼šæ•…éšœç”Ÿæ•ˆåŽç«‹å³æ”¶é›†ï¼ˆé¿å…æ—¥å¿—è¢«è¦†ç›–ï¼‰
- **æ¢å¤åŽ**ï¼šç­‰å¾…10-15ç§’è®©æœåŠ¡ç¨³å®š

### 3. éªŒè¯æ­¥éª¤

- **æ•…éšœæ³¨å…¥åŽ**ï¼šéªŒè¯æ•…éšœç¡®å®žå·²ç”Ÿæ•ˆ
- **æ¢å¤åŽ**ï¼šéªŒè¯é›†ç¾¤å·²å®Œå…¨æ¢å¤
- **ä½¿ç”¨å‘½ä»¤**ï¼š`hdfs dfsadmin -report`ã€`jps`ã€`hdfs dfsadmin -safemode get` ç­‰

### 4. æ–‡ä»¶ç®¡ç†

- **ç»Ÿä¸€å‘½å**ï¼šä½¿ç”¨ `case1`ã€`case2` ç­‰ç¼–å·
- **åŠæ—¶ä¿å­˜**ï¼šæ¯ä¸ªæ­¥éª¤å®ŒæˆåŽç«‹å³ä¿å­˜æ–‡ä»¶
- **å¤‡ä»½é‡è¦æ•°æ®**ï¼šå¤çŽ°å‰å¤‡ä»½å…³é”®é…ç½®å’Œæ•°æ®

### 5. æ•…éšœéš”ç¦»

- **ä¸€æ¬¡ä¸€ä¸ª**ï¼šæ¯æ¬¡åªå¤çŽ°ä¸€ä¸ªæ•…éšœç±»åž‹
- **å®Œå…¨æ¢å¤**ï¼šæ¯ä¸ªæ•…éšœå¤çŽ°åŽå¿…é¡»å®Œå…¨æ¢å¤
- **é—´éš”æ—¶é—´**ï¼šæ•…éšœä¹‹é—´é—´éš”10-15åˆ†é’Ÿï¼Œç¡®ä¿é›†ç¾¤ç¨³å®š

---

## å¿«é€Ÿå‚è€ƒ

### å¸¸ç”¨å‘½ä»¤

```bash
# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -report"'

# æ£€æŸ¥å®‰å…¨æ¨¡å¼
docker exec namenode sh -c 'su - hadoop -c "hdfs dfsadmin -safemode get"'

# æ£€æŸ¥è¿›ç¨‹
docker exec namenode sh -c 'su - hadoop -c "jps"'

# æŸ¥çœ‹æ—¥å¿—ï¼ˆæœ€åŽ200è¡Œï¼‰
docker exec namenode tail -200 /usr/local/hadoop/logs/hadoop-hadoop-namenode-namenode.log
```

### ç›®å½•ç»“æž„

```
test_cases/
â”œâ”€â”€ hdfs/
â”‚   â”œâ”€â”€ datanode_down/
â”‚   â”‚   â””â”€â”€ case1/
â”‚   â”‚       â”œâ”€â”€ cluster_logs.txt    # é›†ç¾¤æ—¥å¿—ï¼ˆDatasetçš„inputï¼‰
â”‚   â”‚       â””â”€â”€ return.txt          # Agentè¯Šæ–­ç»“æžœï¼ˆDatasetçš„outputï¼‰
â”‚   â”œâ”€â”€ namenode_safemode/
â”‚   â”‚   â””â”€â”€ case1/
â”‚   â”‚       â”œâ”€â”€ cluster_logs.txt
â”‚   â”‚       â””â”€â”€ return.txt
â”‚   â””â”€â”€ cluster_id_mismatch/
â”‚       â””â”€â”€ case1/
â”‚           â”œâ”€â”€ cluster_logs.txt
â”‚           â””â”€â”€ return.txt
â”œâ”€â”€ yarn/
â”‚   â”œâ”€â”€ resourcemanager_down/
â”‚   â”‚   â””â”€â”€ case1/
â”‚   â”‚       â”œâ”€â”€ cluster_logs.txt
â”‚   â”‚       â””â”€â”€ return.txt
â”‚   â”œâ”€â”€ nodemanager_down/
â”‚   â””â”€â”€ yarn_config_error/
â””â”€â”€ mapreduce/
    â”œâ”€â”€ mapreduce_memory_insufficient/
    â”œâ”€â”€ mapreduce_disk_insufficient/
    â”œâ”€â”€ mapreduce_shuffle_failed/
    â””â”€â”€ mapreduce_task_timeout/
```

**è¯´æ˜Ž**ï¼š
- æ¯ä¸ª `caseX` ç›®å½•ä¸‹åªéœ€è¦ä¸¤ä¸ªæ–‡ä»¶
- `cluster_logs.txt` å’Œ `return.txt` å¯ä»¥ç›´æŽ¥ç”¨äºŽç”ŸæˆDatasetæ ·æœ¬
- æ•…éšœç±»åž‹ä¿¡æ¯å¯ä»¥ä»Žç›®å½•è·¯å¾„èŽ·å–ï¼ˆå¦‚ï¼š`hdfs/datanode_down`ï¼‰

---

## é‡åˆ°é—®é¢˜ï¼Ÿ

1. **æ•…éšœæœªç”Ÿæ•ˆ**ï¼šæ£€æŸ¥å‘½ä»¤æ˜¯å¦æ­£ç¡®æ‰§è¡Œï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
2. **æ—¥å¿—æ”¶é›†å¤±è´¥**ï¼šæ£€æŸ¥å®¹å™¨æ˜¯å¦è¿è¡Œï¼Œæ—¥å¿—è·¯å¾„æ˜¯å¦æ­£ç¡®
3. **æ¢å¤å¤±è´¥**ï¼šå‚è€ƒ `æ•…éšœå¤çŽ°æ–¹æ¡ˆ.md` ä¸­çš„æ¢å¤æ–¹æ³•
4. **é›†ç¾¤å¼‚å¸¸**ï¼šåœæ­¢æ‰€æœ‰æ“ä½œï¼Œæ‰‹åŠ¨æ¢å¤é›†ç¾¤åˆ°æ­£å¸¸çŠ¶æ€

---

**ç¥å¤çŽ°é¡ºåˆ©ï¼** ðŸš€
